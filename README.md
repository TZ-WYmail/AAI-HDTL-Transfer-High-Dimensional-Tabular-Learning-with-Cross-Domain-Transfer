# AAI Project - é«˜ç»´å°æ ·æœ¬äºŒåˆ†ç±»æ¨¡å‹

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3+-orange.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## é¡¹ç›®ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªé«˜ç»´å°æ ·æœ¬æ•°æ®çš„äºŒåˆ†ç±»æœºå™¨å­¦ä¹ é¡¹ç›®ï¼Œä½¿ç”¨8ä¸ªä¸åŒçš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œæ€§èƒ½å¯¹æ¯”å’Œåˆ†æã€‚é¡¹ç›®åŒ…å«å®Œæ•´çš„æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œå¯è§†åŒ–æµç¨‹ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- ğŸ“Š **é«˜ç»´æ•°æ®å¤„ç†**: ä»12,700ä¸ªç‰¹å¾é™ç»´è‡³500ä¸ªæ ¸å¿ƒç‰¹å¾
- ğŸ¤– **8æ¨¡å‹å¯¹æ¯”**: Logistic Regression, SVM (Linear/RBF), Random Forest, Gradient Boosting, KNN, Naive Bayes, Neural Network
- ğŸ¯ **æœ€ä¼˜ROC-AUC**: 0.9405 (Logistic Regression)
- ğŸ“ˆ **å®Œæ•´Pipeline**: æ•°æ®é¢„å¤„ç† â†’ ç‰¹å¾é€‰æ‹© â†’ æ¨¡å‹è®­ç»ƒ â†’ äº¤å‰éªŒè¯ â†’ ç»“æœåˆ†æ
- ğŸ“ **è¯¦ç»†æ–‡æ¡£**: æ•°æ®åˆ†ææŠ¥å‘Š + æ¨¡å‹æ€§èƒ½åˆ†ææŠ¥å‘Š

---

## ç›®å½•ç»“æ„

```
AAI_Project/
â”œâ”€â”€ data/                           # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ train.csv                  # è®­ç»ƒæ•°æ® (196æ ·æœ¬ Ã— 12,701åˆ—)
â”‚   â”œâ”€â”€ test_in_domain.csv         # åŸŸå†…æµ‹è¯•é›† (84æ ·æœ¬)
â”‚   â””â”€â”€ test_cross_domain.csv      # è·¨åŸŸæµ‹è¯•é›† (200æ ·æœ¬)
â”‚
â”œâ”€â”€ src/                           # æºä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processing.ipynb      # æ•°æ®åˆ†æå’Œå¯è§†åŒ–
â”‚   â””â”€â”€ model_training_and_evaluation.ipynb  # æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
â”‚
â”œâ”€â”€ docs/                          # æ–‡æ¡£
â”‚   â”œâ”€â”€ data_analysis_report.md           # æ•°æ®åˆ†æå®Œæ•´æŠ¥å‘Š
â”‚   â”œâ”€â”€ model_performance_analysis.md     # æ¨¡å‹æ€§èƒ½åˆ†ææŠ¥å‘Š
â”‚   â”œâ”€â”€ data_processing_explanation.md    # æ•°æ®å¤„ç†è¯´æ˜
â”‚   â””â”€â”€ images/                           # å¯è§†åŒ–å›¾è¡¨
â”‚       â”œâ”€â”€ 01_label_distribution.png
â”‚       â”œâ”€â”€ 02_feature_variance_distribution.png
â”‚       â”œâ”€â”€ 03_feature_label_correlation.png
â”‚       â”œâ”€â”€ 04_top_features_distribution.png
â”‚       â”œâ”€â”€ 05_top_features_boxplot.png
â”‚       â”œâ”€â”€ 06_pca_analysis.png
â”‚       â””â”€â”€ 07_model_comparison.png
â”‚
â”œâ”€â”€ requirements.txt               # Pythonä¾èµ–
â”œâ”€â”€ .gitignore                     # Gitå¿½ç•¥æ–‡ä»¶
â””â”€â”€ README.md                      # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

---

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚

- Python 3.12+
- Jupyter Notebook / JupyterLab
- æ¨èä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ

### 2. å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ï¼‰
python -m venv venv

# Windowsæ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
venv\Scripts\activate

# Linux/Macæ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 3. è¿è¡Œæ•°æ®åˆ†æ

```bash
# å¯åŠ¨Jupyter Notebook
jupyter notebook

# æ‰“å¼€å¹¶è¿è¡Œ
src/data_processing.ipynb
```

### 4. è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹

```bash
# è¿è¡Œæ¨¡å‹è®­ç»ƒnotebook
src/model_training_and_evaluation.ipynb
```

---

## æ•°æ®é›†ä¿¡æ¯

### è®­ç»ƒé›† (train.csv)

| å±æ€§ | å€¼ |
|------|-----|
| æ ·æœ¬æ•° | 196 |
| ç‰¹å¾æ•° | 12,700 |
| æ ‡ç­¾åˆ— | 1 (æœ€åä¸€åˆ—) |
| ç±»åˆ«åˆ†å¸ƒ | ç±»åˆ«0: 70 (35.7%), ç±»åˆ«1: 126 (64.3%) |
| ç±»åˆ«æ¯”ä¾‹ | 1:1.8 (è½»å¾®ä¸å¹³è¡¡) |
| ç¼ºå¤±å€¼ | 0 |
| å¸¸é‡ç‰¹å¾ | 0 |

### æµ‹è¯•é›†

- **test_in_domain.csv**: 84ä¸ªæ ·æœ¬ï¼ˆåŸŸå†…æµ‹è¯•ï¼‰
- **test_cross_domain.csv**: 200ä¸ªæ ·æœ¬ï¼ˆè·¨åŸŸæµ‹è¯•ï¼‰

---

## æ ¸å¿ƒåŠŸèƒ½

### 1. æ•°æ®é¢„å¤„ç† Pipeline

```
åŸå§‹æ•°æ® (196 Ã— 12,700)
    â†“
æ–¹å·®è¿‡æ»¤ (threshold=0.01) â†’ ç§»é™¤531ä¸ªä½æ–¹å·®ç‰¹å¾
    â†“
æ ‡å‡†åŒ– (StandardScaler) â†’ Z-scoreå½’ä¸€åŒ–
    â†“
ç‰¹å¾é€‰æ‹© (SelectKBest, k=500) â†’ é€‰æ‹©Top 500ç‰¹å¾
    â†“
æœ€ç»ˆæ•°æ® (196 Ã— 500)
```

**é™ç»´æ¯”ä¾‹**: 96.1%

### 2. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°

#### 8ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹

| æ¨¡å‹ | ROC-AUC | Accuracy | F1-Score | è®­ç»ƒæ—¶é—´ | æ¨èåº¦ |
|------|---------|----------|----------|----------|--------|
| **Logistic Regression** | **0.9405** | **0.8676** | 0.8940 | 0.004s | â­â­â­â­â­ |
| **SVM (Linear)** | **0.9360** | **0.8676** | **0.8960** | 0.009s | â­â­â­â­â­ |
| **SVM (RBF)** | **0.9078** | 0.8065 | 0.8465 | 0.010s | â­â­â­â­ |
| Neural Network (MLP) | 0.8645 | 0.7915 | 0.8342 | 1.107s | â­â­â­â­ |
| Naive Bayes | 0.8424 | 0.7303 | 0.7753 | 0.001s | â­â­â­ |
| Random Forest | 0.8407 | 0.7656 | 0.8355 | 0.258s | â­â­â­ |
| Gradient Boosting | 0.8131 | 0.7603 | 0.8296 | 5.336s | â­â­â­ |
| K-Nearest Neighbors | 0.8018 | 0.7097 | 0.8059 | 0.001s | â­â­ |

#### è¯„ä¼°æŒ‡æ ‡

- **ä¸»æŒ‡æ ‡**: ROC-AUC (å—ç±»åˆ«ä¸å¹³è¡¡å½±å“å°)
- **è¾…åŠ©æŒ‡æ ‡**: Accuracy, F1-Score, Precision, Recall
- **éªŒè¯æ–¹æ³•**: 5æŠ˜åˆ†å±‚äº¤å‰éªŒè¯ (StratifiedKFold)

### 3. æœ€ä¼˜æ¨¡å‹é…ç½®

**Logistic Regression** (ROC-AUC: 0.9405) ğŸ†

```python
LogisticRegression(
    penalty='l2',              # L2æ­£åˆ™åŒ–
    C=0.1,                    # å¼ºæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
    max_iter=2000,            # æœ€å¤§è¿­ä»£æ¬¡æ•°
    class_weight='balanced',   # è‡ªåŠ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    random_state=42
)
```

**ä¸ºä»€ä¹ˆLogistic Regressionæœ€ä¼˜ï¼Ÿ**
- âœ… æœ€é«˜ROC-AUC (0.9405)
- âœ… æœ€ç¨³å®š (Std=0.0179)
- âœ… è®­ç»ƒå¿«é€Ÿ (0.004ç§’)
- âœ… é«˜å¯è§£é‡Šæ€§

---

## ä¸»è¦å‘ç°

### æ•°æ®ç‰¹å¾

- âœ… **æ•°æ®è´¨é‡è‰¯å¥½**: æ— ç¼ºå¤±å€¼ã€æ— å¸¸é‡ç‰¹å¾
- âš ï¸ **é«˜ç»´åº¦æŒ‘æˆ˜**: 12,700ä¸ªç‰¹å¾ï¼Œè¿œè¶…æ ·æœ¬æ•°
- âš ï¸ **å°æ ·æœ¬é™åˆ¶**: ä»…196ä¸ªè®­ç»ƒæ ·æœ¬
- âš ï¸ **ä½ä¿¡å™ªæ¯”**: 71%çš„ç‰¹å¾ä¸æ ‡ç­¾ç›¸å…³æ€§<0.1
- âœ… **è½»å¾®ç±»åˆ«ä¸å¹³è¡¡**: 1:1.8æ¯”ä¾‹ï¼Œå¯å¤„ç†

### æ¨¡å‹æ€§èƒ½

1. **çº¿æ€§æ¨¡å‹æ˜¾è‘—ä¼˜äºéçº¿æ€§æ¨¡å‹**
   - å‰3åå‡ä¸ºçº¿æ€§æˆ–æ ¸æ–¹æ³•
   - Logistic Regressionå’ŒSVM (Linear)æ€§èƒ½æ¥è¿‘

2. **ç®€å•å¾€å¾€æ›´å¥½** (Occam's Razor)
   - æœ€ç®€å•çš„Logistic Regressionå‡»è´¥æ‰€æœ‰å¤æ‚æ¨¡å‹
   - æ­£åˆ™åŒ–æ¯”æ¨¡å‹å¤æ‚åº¦æ›´é‡è¦

3. **è¿‡æ‹Ÿåˆé£é™©**
   - è®­ç»ƒé›†ä¸éªŒè¯é›†å­˜åœ¨è½»å¾®å·®è·
   - å¼ºæ­£åˆ™åŒ–(C=0.1)æ˜¯å…³é”®

---

## ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒæµç¨‹

```python
import pandas as pd
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# 1. åŠ è½½æ•°æ®
train_df = pd.read_csv('data/train.csv')
X = train_df.iloc[:, :-1].values
y = train_df.iloc[:, -1].values

# 2. æ„å»ºPipeline
pipeline = Pipeline([
    ('variance_filter', VarianceThreshold(threshold=0.01)),
    ('scaler', StandardScaler()),
    ('selector', SelectKBest(f_classif, k=500)),
    ('classifier', LogisticRegression(C=0.1, max_iter=2000, 
                                     class_weight='balanced', 
                                     random_state=42))
])

# 3. äº¤å‰éªŒè¯
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_results = cross_validate(pipeline, X, y, cv=cv, 
                           scoring='roc_auc', return_train_score=True)

print(f"ROC-AUC: {cv_results['test_score'].mean():.4f} Â± {cv_results['test_score'].std():.4f}")

# 4. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
pipeline.fit(X, y)

# 5. é¢„æµ‹æµ‹è¯•é›†
test_df = pd.read_csv('data/test_in_domain.csv')
predictions = pipeline.predict_proba(test_df.values)[:, 1]

# 6. ä¿å­˜ç»“æœ
pd.DataFrame({'prediction_proba': predictions}).to_csv('predictions_in_domain.csv', index=False)
```

### å¤šæ¨¡å‹æ¯”è¾ƒ

```python
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

models = {
    'Logistic Regression': LogisticRegression(C=0.1, max_iter=2000, random_state=42),
    'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)
}

for name, model in models.items():
    pipeline = Pipeline([
        ('variance_filter', VarianceThreshold(threshold=0.01)),
        ('scaler', StandardScaler()),
        ('selector', SelectKBest(f_classif, k=500)),
        ('classifier', model)
    ])
    
    scores = cross_val_score(pipeline, X, y, cv=5, scoring='roc_auc')
    print(f"{name}: {scores.mean():.4f} Â± {scores.std():.4f}")
```

---

## æ–‡æ¡£

### è¯¦ç»†åˆ†ææŠ¥å‘Š

1. **[æ•°æ®åˆ†ææŠ¥å‘Š](docs/data_analysis_report.md)** (1,500+ è¡Œ)
   - æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
   - ç‰¹å¾åˆ†å¸ƒå’Œç›¸å…³æ€§åˆ†æ
   - PCAé™ç»´åˆ†æ
   - å»ºæ¨¡ç­–ç•¥å»ºè®®
   - å®Œæ•´Pipelineç¤ºä¾‹

2. **[æ¨¡å‹æ€§èƒ½åˆ†ææŠ¥å‘Š](docs/model_performance_analysis.md)** (500+ è¡Œ)
   - 8ä¸ªæ¨¡å‹è¯¦ç»†å¯¹æ¯”
   - æ€§èƒ½æŒ‡æ ‡å¤šç»´åº¦åˆ†æ
   - æœ€ä¼˜æ¨¡å‹æ·±åº¦å‰–æ
   - åœºæ™¯åŒ–æ¨¡å‹é€‰æ‹©å»ºè®®
   - æ€§èƒ½ä¼˜åŒ–è·¯çº¿å›¾

### å¯è§†åŒ–å›¾è¡¨

- æ ‡ç­¾åˆ†å¸ƒå›¾
- ç‰¹å¾æ–¹å·®åˆ†å¸ƒå›¾
- ç‰¹å¾-æ ‡ç­¾ç›¸å…³æ€§çƒ­å›¾
- Topç‰¹å¾åˆ†å¸ƒå¯¹æ¯”
- PCAé™ç»´åˆ†æå›¾
- æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾ï¼ˆ4ç»´åº¦ï¼‰

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### çŸ­æœŸä¼˜åŒ– (å¿«é€Ÿè§æ•ˆ)

1. **è¶…å‚æ•°ç²¾ç»†è°ƒä¼˜**
   ```python
   from sklearn.model_selection import GridSearchCV
   
   param_grid = {
       'C': [0.01, 0.05, 0.1, 0.5, 1.0],
       'penalty': ['l1', 'l2']
   }
   
   grid_search = GridSearchCV(LogisticRegression(max_iter=2000), 
                              param_grid, cv=5, scoring='roc_auc')
   ```
   **é¢„æœŸæå‡**: +0.005-0.01 ROC-AUC

2. **ç‰¹å¾æ•°é‡ä¼˜åŒ–**
   - æµ‹è¯•kå€¼: 200, 300, 400, 500, 600, 800
   - å½“å‰k=500å¯èƒ½ä¸æ˜¯æœ€ä¼˜

3. **é›†æˆå­¦ä¹ **
   ```python
   from sklearn.ensemble import VotingClassifier
   
   ensemble = VotingClassifier([
       ('lr', LogisticRegression(C=0.1)),
       ('svm', SVC(kernel='linear', probability=True))
   ], voting='soft')
   ```
   **é¢„æœŸæå‡**: +0.01-0.02 ROC-AUC

### ä¸­æœŸä¼˜åŒ–

- SMOTEè¿‡é‡‡æ ·å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
- ç‰¹å¾äº¤äº’é¡¹ç”Ÿæˆ
- æ·±åº¦å­¦ä¹ æ¶æ„ä¼˜åŒ–

### é•¿æœŸä¼˜åŒ–

- é‡æ–°è®¾è®¡ç‰¹å¾é€‰æ‹©ç­–ç•¥ (RFECV)
- æ¢ç´¢å…¶ä»–é™ç»´æ–¹æ³• (PCA, UMAP)
- é«˜çº§é›†æˆç­–ç•¥ (Stacking)

---

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆä¸ä½¿ç”¨æ·±åº¦å­¦ä¹ ï¼Ÿ

**A**: 
- æ ·æœ¬é‡å¤ªå°ï¼ˆ196ä¸ªï¼‰ï¼Œæ·±åº¦å­¦ä¹ å®¹æ˜“è¿‡æ‹Ÿåˆ
- ç¥ç»ç½‘ç»œ(MLP)è¡¨ç°ä¸­ç­‰ï¼ˆROC-AUC 0.8645ï¼‰ï¼Œä¸å¦‚ç®€å•çš„Logistic Regression
- è®­ç»ƒæ—¶é—´é•¿ï¼ˆ1.1ç§’ vs 0.004ç§’ï¼‰

### Q2: å¦‚ä½•å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Ÿ

**A**: 
- ä½¿ç”¨`class_weight='balanced'`è‡ªåŠ¨è°ƒæ•´æƒé‡
- è¯„ä¼°æŒ‡æ ‡é€‰æ‹©ROC-AUCï¼ˆå¯¹ä¸å¹³è¡¡ä¸æ•æ„Ÿï¼‰
- å¯é€‰ï¼šSMOTEè¿‡é‡‡æ ·ï¼ˆéœ€è¦æ³¨æ„æ•°æ®æ³„éœ²ï¼‰

### Q3: ä¸ºä»€ä¹ˆé™ç»´è¿™ä¹ˆé‡è¦ï¼Ÿ

**A**: 
- ç‰¹å¾æ•°(12,700) >> æ ·æœ¬æ•°(196)ï¼Œç»´åº¦è¯…å’’ä¸¥é‡
- ä¸é™ç»´ä¼šå¯¼è‡´ä¸¥é‡è¿‡æ‹Ÿåˆ
- é™ç»´è‡³500ç»´åï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡

### Q4: å¦‚ä½•é¿å…æ•°æ®æ³„éœ²ï¼Ÿ

**A**: 
- ä½¿ç”¨sklearnçš„Pipeline
- ç‰¹å¾é€‰æ‹©åœ¨äº¤å‰éªŒè¯çš„æ¯ä¸€æŠ˜ä¸­ç‹¬ç«‹è¿›è¡Œ
- æµ‹è¯•é›†ä¸å‚ä¸ä»»ä½•è®­ç»ƒè¿‡ç¨‹

---

## è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ”¹è¿›å»ºè®®ï¼

### å¦‚ä½•è´¡çŒ®

1. Fork æœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

---

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## è”ç³»æ–¹å¼

- **é¡¹ç›®ä½œè€…**: AAI Team
- **åˆ›å»ºæ—¥æœŸ**: 2025å¹´12æœˆ7æ—¥
- **æœ€åæ›´æ–°**: 2025å¹´12æœˆ7æ—¥

---

## è‡´è°¢

- scikit-learn å›¢é˜Ÿæä¾›çš„ä¼˜ç§€æœºå™¨å­¦ä¹ åº“
- matplotlib å’Œ seaborn æä¾›çš„å¯è§†åŒ–å·¥å…·
- Jupyter é¡¹ç›®æä¾›çš„äº¤äº’å¼å¼€å‘ç¯å¢ƒ

---

## æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-12-07)
- âœ¨ åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- âœ… å®Œæˆæ•°æ®åˆ†æå’Œå¯è§†åŒ–
- âœ… è®­ç»ƒå’Œè¯„ä¼°8ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹
- âœ… ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š
- âœ… ç¡®å®šæœ€ä¼˜æ¨¡å‹ (Logistic Regression, ROC-AUC: 0.9405)

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸€ä¸ªStarï¼**
