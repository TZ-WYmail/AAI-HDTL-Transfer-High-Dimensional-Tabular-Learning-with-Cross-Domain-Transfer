# æ•°æ®å¤„ç†ä¸åˆ†æå®Œæ•´æŠ¥å‘Š

## ç›®å½•
1. [é¡¹ç›®æ¦‚è§ˆ](#é¡¹ç›®æ¦‚è§ˆ)
2. [ç¯å¢ƒå‡†å¤‡ä¸æ•°æ®åŠ è½½](#ç¯å¢ƒå‡†å¤‡ä¸æ•°æ®åŠ è½½)
3. [æ•°æ®é›†åŸºæœ¬ä¿¡æ¯](#æ•°æ®é›†åŸºæœ¬ä¿¡æ¯)
4. [æ ‡ç­¾åˆ†å¸ƒåˆ†æ](#æ ‡ç­¾åˆ†å¸ƒåˆ†æ)
5. [ç‰¹å¾æ–¹å·®åˆ†æ](#ç‰¹å¾æ–¹å·®åˆ†æ)
6. [ç‰¹å¾-æ ‡ç­¾ç›¸å…³æ€§åˆ†æ](#ç‰¹å¾-æ ‡ç­¾ç›¸å…³æ€§åˆ†æ)
7. [Topç‰¹å¾è¯¦ç»†åˆ†æ](#topç‰¹å¾è¯¦ç»†åˆ†æ)
8. [PCAé™ç»´åˆ†æ](#pcaé™ç»´åˆ†æ)
9. [ç»¼åˆæ•°æ®æ´å¯Ÿ](#ç»¼åˆæ•°æ®æ´å¯Ÿ)
10. [å»ºæ¨¡ç­–ç•¥å»ºè®®](#å»ºæ¨¡ç­–ç•¥å»ºè®®)
11. [å®Œæ•´å»ºæ¨¡Pipelineç¤ºä¾‹](#å®Œæ•´å»ºæ¨¡pipelineç¤ºä¾‹)
12. [è¿›é˜¶æŠ€å·§](#è¿›é˜¶æŠ€å·§)
13. [å¸¸è§é™·é˜±ä¸æ³¨æ„äº‹é¡¹](#å¸¸è§é™·é˜±ä¸æ³¨æ„äº‹é¡¹)
14. [é¢„æœŸæ€§èƒ½åŸºå‡†](#é¢„æœŸæ€§èƒ½åŸºå‡†)
15. [æ€»ç»“](#æ€»ç»“)
16. [é™„å½•](#é™„å½•)

---

## é¡¹ç›®æ¦‚è§ˆ

æœ¬æŠ¥å‘Šè¯¦ç»†åˆ†æäº†ä¸€ä¸ªå…¸å‹çš„**é«˜ç»´å°æ ·æœ¬äºŒåˆ†ç±»é—®é¢˜**æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ã€åŸºå› ç»„å­¦ã€åŒ»å­¦è¯Šæ–­ç­‰é¢†åŸŸéå¸¸å¸¸è§ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š

### æ•°æ®è§„æ¨¡ç‰¹å¾

- **æ ·æœ¬æ•°é‡**: 196ä¸ªæ ·æœ¬
- **ç‰¹å¾æ•°é‡**: 12,700ä¸ªç‰¹å¾  
- **ç‰¹å¾/æ ·æœ¬æ¯”**: 64.8:1
- **ä»»åŠ¡ç±»å‹**: äºŒåˆ†ç±»é—®é¢˜

### é«˜ç»´å°æ ·æœ¬é—®é¢˜çš„æŒ‘æˆ˜

è¿™ç§æ•°æ®ç‰¹å¾åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- **ç»´åº¦è¯…å’’**: ç‰¹å¾æ•°è¿œå¤§äºæ ·æœ¬æ•°ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ
- **è®¡ç®—å¤æ‚åº¦**: é«˜ç»´æ•°æ®å¤„ç†å’Œå»ºæ¨¡è€—æ—¶
- **ç‰¹å¾å†—ä½™**: å¤§é‡ç‰¹å¾å¯èƒ½åŒ…å«å™ªå£°æˆ–é‡å¤ä¿¡æ¯
- **æ¨¡å‹é€‰æ‹©å—é™**: éœ€è¦ç‰¹æ®Šçš„å»ºæ¨¡ç­–ç•¥

---

## ç¯å¢ƒå‡†å¤‡ä¸æ•°æ®åŠ è½½

### 1. å¯¼å…¥å¿…è¦çš„åº“

```python
import pandas as pd              # æ•°æ®å¤„ç†
import numpy as np               # æ•°å€¼è®¡ç®—
import matplotlib.pyplot as plt  # åŸºç¡€ç»˜å›¾
import seaborn as sns            # é«˜çº§å¯è§†åŒ–
from sklearn.preprocessing import StandardScaler  # æ•°æ®æ ‡å‡†åŒ–
from sklearn.decomposition import PCA            # é™ç»´
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
import warnings
import os
```

### 2. ç¯å¢ƒé…ç½®

```python
# åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
os.makedirs('../docs/images', exist_ok=True)

# è®¾ç½®ç¾è§‚çš„ç»˜å›¾é£æ ¼
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.sans-serif'] = ['SimHei']  # æ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['axes.unicode_minus'] = False    # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')
```

**å…³é”®é…ç½®è¯´æ˜**:
- `sns.set_style('whitegrid')`: è®¾ç½®æ¸…æ™°çš„ç½‘æ ¼èƒŒæ™¯ï¼Œä¾¿äºè¯»å›¾
- `warnings.filterwarnings('ignore')`: æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Šä¿¡æ¯
- å›¾ç‰‡ä¿å­˜è‡³`docs/images/`ç›®å½•ï¼Œä¾¿äºæ–‡æ¡£å¼•ç”¨

### 3. åŠ è½½æ•°æ®é›†

```python
# å®šä¹‰æ–‡ä»¶è·¯å¾„
TRAIN_PATH = '../data/train.csv'
TEST_IN_DOMAIN_PATH = '../data/test_in_domain.csv'
TEST_CROSS_DOMAIN_PATH = '../data/test_cross_domain.csv'

# åŠ è½½æ•°æ®
train_df = pd.read_csv(TRAIN_PATH)
test_in_df = pd.read_csv(TEST_IN_DOMAIN_PATH)
test_cross_df = pd.read_csv(TEST_CROSS_DOMAIN_PATH)
```

**ä¸‰ä¸ªæ•°æ®é›†çš„ä½œç”¨**:

| æ•°æ®é›† | æ–‡ä»¶å | ç”¨é€” | é‡è¦æ€§ |
|--------|--------|------|--------|
| **è®­ç»ƒé›†** | train.csv | æ¨¡å‹è®­ç»ƒå’Œç‰¹å¾åˆ†æ | â­â­â­â­â­ |
| **åŸŸå†…æµ‹è¯•é›†** | test_in_domain.csv | è¯„ä¼°åœ¨ç›¸ä¼¼æ•°æ®ä¸Šçš„è¡¨ç° | â­â­â­â­ |
| **è·¨åŸŸæµ‹è¯•é›†** | test_cross_domain.csv | è¯„ä¼°æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ | â­â­â­â­â­ |

---

## æ•°æ®é›†åŸºæœ¬ä¿¡æ¯

### åŸºç¡€ä¿¡æ¯æ£€æŸ¥æµç¨‹

åœ¨è¿›è¡Œä»»ä½•åˆ†æä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç³»ç»Ÿåœ°æ£€æŸ¥æ•°æ®è´¨é‡ï¼š

#### 1. æ•°æ®å½¢çŠ¶æ£€æŸ¥
```python
print(f"Training data shape: {train_df.shape}")
print(f"In-domain test data shape: {test_in_df.shape}")
print(f"Cross-domain test data shape: {test_cross_df.shape}")
```

**å®é™…ç»“æœ**:
- è®­ç»ƒé›†: (196, 12701) - 196ä¸ªæ ·æœ¬ï¼Œ12700ä¸ªç‰¹å¾ + 1ä¸ªæ ‡ç­¾
- åŸŸå†…æµ‹è¯•é›†: è¯„ä¼°ç›¸ä¼¼æ•°æ®æ€§èƒ½
- è·¨åŸŸæµ‹è¯•é›†: è¯„ä¼°æ³›åŒ–èƒ½åŠ›

**ç›®çš„**: ç¡®è®¤æ ·æœ¬æ•°å’Œç‰¹å¾æ•°ï¼Œè¯†åˆ«æ•°æ®ç»´åº¦é—®é¢˜

#### 2. åˆ—åæ£€æŸ¥
```python
print(train_df.columns.tolist())
```

**ç›®çš„**: 
- ç¡®è®¤ç‰¹å¾å‘½åè§„åˆ™ï¼ˆæœ¬æ•°æ®é›†ä½¿ç”¨f0, f1, f2...å‘½åï¼‰
- è¯†åˆ«æ ‡ç­¾åˆ—ï¼ˆé€šå¸¸æ˜¯æœ€åä¸€åˆ—ï¼‰
- æ£€æŸ¥æ˜¯å¦æœ‰IDåˆ—æˆ–å…¶ä»–éç‰¹å¾åˆ—

#### 3. æ•°æ®ç±»å‹æ£€æŸ¥
```python
print(train_df.dtypes.value_counts())
```

**å®é™…ç»“æœ**: å…¨éƒ¨ä¸ºæ•°å€¼å‹ï¼ˆfloat64ï¼‰

**ç›®çš„**: ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼å‹ï¼Œä¸éœ€è¦ç¼–ç è½¬æ¢

#### 4. ç¼ºå¤±å€¼æ£€æŸ¥
```python
missing_values = train_df.isnull().sum()
print(f"Number of features with missing values: {(missing_values > 0).sum()}")
```

**å®é™…ç»“æœ**: æ— ç¼ºå¤±å€¼

**ç›®çš„**: è¯†åˆ«éœ€è¦å¡«è¡¥æˆ–åˆ é™¤çš„ç¼ºå¤±æ•°æ®ã€‚æœ¬æ•°æ®é›†æ— éœ€å¤„ç†ç¼ºå¤±å€¼ã€‚

#### 5. æ ‡ç­¾åˆ†å¸ƒæ£€æŸ¥
```python
label_col = train_df.columns[-1]
label_counts = train_df[label_col].value_counts(normalize=True)
print("\nLabel distribution:")
print(label_counts)
```

**å®é™…ç»“æœ**:
- ç±»åˆ«0: 70ä¸ªæ ·æœ¬ï¼ˆ35.7%ï¼‰
- ç±»åˆ«1: 126ä¸ªæ ·æœ¬ï¼ˆ64.3%ï¼‰

**ç›®çš„**: 
- æ£€æŸ¥ç±»åˆ«æ˜¯å¦å¹³è¡¡
- å¦‚æœä¸¥é‡ä¸å¹³è¡¡ï¼ˆå¦‚90:10ï¼‰ï¼Œéœ€è¦ï¼š
  - SMOTEè¿‡é‡‡æ ·
  - ç±»åˆ«æƒé‡è°ƒæ•´ï¼ˆclass_weight='balanced'ï¼‰
  - åˆ†å±‚é‡‡æ ·

### è®­ç»ƒé›†ç»Ÿè®¡æ‘˜è¦

| ç»Ÿè®¡é¡¹ | æ•°å€¼ | çŠ¶æ€ |
|--------|------|------|
| **æ€»æ ·æœ¬æ•°** | 196 | âš ï¸ å°æ ·æœ¬ |
| **æ€»ç‰¹å¾æ•°** | 12,700 | ğŸ”´ è¶…é«˜ç»´ |
| **ç‰¹å¾/æ ·æœ¬æ¯”** | 64.8:1 | ğŸ”´ ä¸¥é‡å¤±è¡¡ |
| **æ•°æ®ç±»å‹** | float64 | âœ… æ•°å€¼å‹ |
| **ç¼ºå¤±å€¼** | 0 | âœ… æ— ç¼ºå¤± |
| **æ ‡ç­¾åˆ—** | æœ€åä¸€åˆ— | âœ… å·²è¯†åˆ« |
| **ç±»åˆ«å¹³è¡¡** | 35.7% vs 64.3% | âš ï¸ è½»å¾®ä¸å¹³è¡¡ |

---

## æ ‡ç­¾åˆ†å¸ƒåˆ†æ

### å¯è§†åŒ–ä»£ç 

```python
# è·å–æ ‡ç­¾åˆ—
label_col = train_df.columns[-1]

# åˆ›å»ºå›¾è¡¨
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# è®¡æ•°å›¾
label_counts = train_df[label_col].value_counts()
sns.countplot(x=label_col, data=train_df, ax=axes[0], palette='Set2')
axes[0].set_title('Label Distribution (Count)', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Class', fontsize=12)
axes[0].set_ylabel('Count', fontsize=12)
for i, v in enumerate(label_counts):
    axes[0].text(i, v + 2, str(v), ha='center', fontweight='bold')

# é¥¼å›¾
label_counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', colors=['#8dd3c7', '#fb8072'])
axes[1].set_title('Label Distribution (Percentage)', fontsize=14, fontweight='bold')
axes[1].set_ylabel('')

plt.tight_layout()
plt.savefig('../docs/images/01_label_distribution.png', dpi=300, bbox_inches='tight')
plt.show()
```

### åˆ†æç»“æœ

![æ ‡ç­¾åˆ†å¸ƒ](images/01_label_distribution.png)

**å…³é”®å‘ç°**:

- **ç±»åˆ«0**: 70ä¸ªæ ·æœ¬ï¼ˆ35.7%ï¼‰
- **ç±»åˆ«1**: 126ä¸ªæ ·æœ¬ï¼ˆ64.3%ï¼‰
- **ç±»åˆ«æ¯”ä¾‹**: çº¦ 1:1.8

### æ·±å…¥åˆ†æ

**ä¸ºä»€ä¹ˆå…³æ³¨ç±»åˆ«å¹³è¡¡ï¼Ÿ**

1. **å½±å“æ¨¡å‹è®­ç»ƒ**:
   - ä¸å¹³è¡¡æ•°æ®ä¼šå¯¼è‡´æ¨¡å‹åå‘å¤šæ•°ç±»
   - å¯èƒ½äº§ç”Ÿ"è™šå‡çš„é«˜å‡†ç¡®ç‡"ï¼ˆæ€»æ˜¯é¢„æµ‹å¤šæ•°ç±»ï¼‰

2. **è¯„ä¼°æŒ‡æ ‡é€‰æ‹©**:
   - å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šå…·æœ‰è¯¯å¯¼æ€§
   - åº”ä½¿ç”¨ROC-AUCã€F1-Scoreç­‰æŒ‡æ ‡

3. **æœ¬æ•°æ®é›†çš„æƒ…å†µ**:
   - âœ… **è½»å¾®ä¸å¹³è¡¡**: 64.3% vs 35.7%ä¸ç®—ä¸¥é‡
   - âœ… **å¯æ¥å—èŒƒå›´**: æ¯”ä¾‹åœ¨1:2ä»¥å†…
   - âš ï¸ **ä»éœ€æ³¨æ„**: å»ºæ¨¡æ—¶åº”é‡‡å–é¢„é˜²æªæ–½

### å¤„ç†å»ºè®®

**æ¨èç­–ç•¥**ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰:

1. **ä½¿ç”¨åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯** â­â­â­â­â­
   ```python
   from sklearn.model_selection import StratifiedKFold
   skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
   ```
   - ç¡®ä¿æ¯æŠ˜éƒ½ä¿æŒç±»åˆ«æ¯”ä¾‹
   - æœ€åŸºç¡€ä¹Ÿæœ€é‡è¦çš„ç­–ç•¥

2. **ä½¿ç”¨ROC-AUCè¯„ä¼°æŒ‡æ ‡** â­â­â­â­â­
   ```python
   from sklearn.metrics import roc_auc_score
   scoring = 'roc_auc'
   ```
   - å¯¹ä¸å¹³è¡¡ä¸æ•æ„Ÿ
   - è¯„ä¼°æ•´ä½“æ’åºèƒ½åŠ›

3. **ç±»åˆ«æƒé‡è°ƒæ•´** â­â­â­â­
   ```python
   model = LogisticRegression(class_weight='balanced')
   ```
   - è‡ªåŠ¨æ ¹æ®ç±»åˆ«é¢‘ç‡è°ƒæ•´æƒé‡
   - ç®€å•æœ‰æ•ˆ

4. **SMOTEè¿‡é‡‡æ ·** â­â­â­ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
   ```python
   from imblearn.over_sampling import SMOTE
   smote = SMOTE(random_state=42)
   X_resampled, y_resampled = smote.fit_resample(X, y)
   ```
   - âš ï¸ æ ·æœ¬å·²ç»å¾ˆå°‘ï¼ˆ196ä¸ªï¼‰ï¼Œè¿‡é‡‡æ ·å¯èƒ½å¼•å…¥å™ªå£°
   - ä»…åœ¨å…¶ä»–æ–¹æ³•æ•ˆæœä¸ä½³æ—¶è€ƒè™‘

**ä¸æ¨èçš„ç­–ç•¥**:
- âŒ æ¬ é‡‡æ ·ï¼šæ ·æœ¬å¤ªå°‘ï¼Œä¸èƒ½å†å‡å°‘
- âŒ å¿½ç•¥ä¸å¹³è¡¡ï¼šè™½ç„¶ä¸ä¸¥é‡ï¼Œä½†åº”ä¸»åŠ¨å¤„ç†

---

## ç‰¹å¾æ–¹å·®åˆ†æ

### ç†è®ºåŸºç¡€ â­ æ ¸å¿ƒæ¦‚å¿µ

**ä»€ä¹ˆæ˜¯æ–¹å·®ï¼Ÿ**

æ–¹å·®è¡¡é‡ç‰¹å¾å€¼çš„ç¦»æ•£ç¨‹åº¦ï¼š
```
var = Î£(xi - mean)Â² / n
```

**æ–¹å·®çš„æ„ä¹‰**:
- **é«˜æ–¹å·®**: ç‰¹å¾å€¼å˜åŒ–å¤§ï¼Œä¿¡æ¯é‡ä¸°å¯Œï¼Œå¯èƒ½æœ‰ç”¨
- **ä½æ–¹å·®**: ç‰¹å¾å€¼å˜åŒ–å°ï¼Œä¿¡æ¯é‡æœ‰é™
- **é›¶æ–¹å·®**: æ‰€æœ‰æ ·æœ¬å€¼ç›¸åŒï¼ˆå¸¸é‡ï¼‰ï¼Œå®Œå…¨æ— ç”¨

**ä¸ºä»€ä¹ˆè¦åˆ†ææ–¹å·®ï¼Ÿ**

1. **è¯†åˆ«æ— ç”¨ç‰¹å¾**: é›¶æ–¹å·®æˆ–æä½æ–¹å·®ç‰¹å¾åº”è¯¥åˆ é™¤
2. **é™ç»´é¢„å¤„ç†**: ä¼˜å…ˆä¿ç•™é«˜æ–¹å·®ç‰¹å¾
3. **è®¡ç®—æ•ˆç‡**: å‡å°‘æ— æ•ˆç‰¹å¾å¯åŠ å¿«è®­ç»ƒé€Ÿåº¦

### åˆ†æä»£ç 

```python
# è®¡ç®—æ‰€æœ‰ç‰¹å¾çš„æ–¹å·®
feature_cols = train_df.columns[:-1]  # æ’é™¤æ ‡ç­¾åˆ—
feature_vars = train_df[feature_cols].var().sort_values(ascending=False)

# ç»Ÿè®¡ä¸åŒæ–¹å·®èŒƒå›´çš„ç‰¹å¾æ•°é‡
print(f"Features with variance = 0: {(feature_vars == 0).sum()}")
print(f"Features with variance < 0.01: {(feature_vars < 0.01).sum()}")
print(f"Features with variance < 0.1: {(feature_vars < 0.1).sum()}")

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(16, 5))

# æ–¹å·®åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
axes[0].hist(np.log10(feature_vars + 1e-10), bins=50, color='steelblue', 
             edgecolor='black', alpha=0.7)
axes[0].set_title('Distribution of Feature Variances (log scale)', 
                   fontsize=13, fontweight='bold')
axes[0].set_xlabel('log10(Variance)')
axes[0].set_ylabel('Number of Features')
axes[0].grid(True, alpha=0.3)

# Top 50 é«˜æ–¹å·®ç‰¹å¾
top_50_vars = feature_vars.head(50)
axes[1].barh(range(len(top_50_vars)), top_50_vars.values, color='coral')
axes[1].set_title('Top 50 Features by Variance', fontsize=13, fontweight='bold')
axes[1].set_xlabel('Variance')
axes[1].set_ylabel('Feature Rank')
axes[1].invert_yaxis()
axes[1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig('../docs/images/02_feature_variance.png', dpi=300, bbox_inches='tight')
plt.show()
```

### åˆ†æç»“æœ

![ç‰¹å¾æ–¹å·®](images/02_feature_variance.png)

**ç»Ÿè®¡æ‘˜è¦**:

| æ–¹å·®èŒƒå›´ | ç‰¹å¾æ•°é‡ | å æ¯” | è¯„ä»· |
|----------|----------|------|------|
| = 0ï¼ˆå¸¸é‡ç‰¹å¾ï¼‰ | 0 | 0% | âœ… ä¼˜ç§€ |
| < 0.01ï¼ˆæä½æ–¹å·®ï¼‰ | 531 | 4.2% | âš ï¸ å¯åˆ é™¤ |
| < 0.1ï¼ˆä½æ–¹å·®ï¼‰ | 7,378 | 58.1% | âš ï¸ ä¿¡æ¯é‡æœ‰é™ |
| â‰¥ 0.1ï¼ˆæ­£å¸¸æ–¹å·®ï¼‰ | 5,322 | 41.9% | âœ… ä¿ç•™ |

### æ·±å…¥è§£è¯»

**1. å·¦å›¾ï¼šæ–¹å·®åˆ†å¸ƒï¼ˆå¯¹æ•°å°ºåº¦ï¼‰**

- **ä¸ºä»€ä¹ˆç”¨å¯¹æ•°å°ºåº¦ï¼Ÿ** 
  - æ–¹å·®è·¨åº¦å¾ˆå¤§ï¼ˆä»0.001åˆ°30+ï¼‰
  - å¯¹æ•°å°ºåº¦èƒ½æ›´æ¸…æ™°åœ°æ˜¾ç¤ºåˆ†å¸ƒå½¢æ€
  
- **è§‚å¯Ÿç»“æœ**:
  - å‘ˆå¯¹æ•°æ­£æ€åˆ†å¸ƒ
  - å¤§éƒ¨åˆ†ç‰¹å¾é›†ä¸­åœ¨ä½æ–¹å·®åŒºåŸŸï¼ˆlog10(var) â‰ˆ -1ï¼‰
  - å°‘æ•°ç‰¹å¾å…·æœ‰å¾ˆé«˜çš„æ–¹å·®

**2. å³å›¾ï¼šTop 50é«˜æ–¹å·®ç‰¹å¾**

- **è§‚å¯Ÿç»“æœ**:
  - æœ€é«˜æ–¹å·®çº¦ä¸º30
  - Top 50ç‰¹å¾çš„æ–¹å·®è¿œé«˜äºå¹³å‡æ°´å¹³
  - è¿™äº›ç‰¹å¾å¯èƒ½åŒ…å«æœ€é‡è¦çš„ä¿¡æ¯

**3. å…³é”®å‘ç°**

1. âœ… **æ— å¸¸é‡ç‰¹å¾**: 
   - æ²¡æœ‰æ–¹å·®ä¸º0çš„ç‰¹å¾
   - è¯´æ˜æ•°æ®é¢„å¤„ç†åšå¾—å¥½

2. âš ï¸ **å¤§é‡ä½æ–¹å·®ç‰¹å¾**: 
   - 58.1%çš„ç‰¹å¾æ–¹å·® < 0.1
   - è¿™äº›ç‰¹å¾æä¾›çš„ä¿¡æ¯é‡æœ‰é™
   - å¯ä»¥è€ƒè™‘åˆ é™¤ä»¥é™ç»´

3. ğŸ“Š **æ–¹å·®åˆ†å¸ƒä¸å‡**:
   - å°‘æ•°ç‰¹å¾æ–¹å·®å¾ˆå¤§
   - å¤šæ•°ç‰¹å¾æ–¹å·®è¾ƒå°
   - ç¬¦åˆ"é•¿å°¾åˆ†å¸ƒ"ç‰¹å¾

### ç‰¹å¾é€‰æ‹©å»ºè®®

**ç­–ç•¥1: åˆ é™¤æä½æ–¹å·®ç‰¹å¾ï¼ˆä¿å®ˆï¼‰**
```python
from sklearn.feature_selection import VarianceThreshold

# åˆ é™¤æ–¹å·® < 0.01 çš„ç‰¹å¾
selector = VarianceThreshold(threshold=0.01)
X_filtered = selector.fit_transform(X)
# å¯å‡å°‘ 531 ä¸ªç‰¹å¾ (12700 -> 12169)

print(f"ä¿ç•™ç‰¹å¾æ•°: {X_filtered.shape[1]}")
```

**ç­–ç•¥2: ä¿ç•™é«˜æ–¹å·®ç‰¹å¾ï¼ˆæ¿€è¿›ï¼‰**
```python
# ä¿ç•™æ–¹å·® >= 0.1 çš„ç‰¹å¾
high_var_features = feature_vars[feature_vars >= 0.1].index
X_selected = train_df[high_var_features]
# ä¿ç•™ 5,322 ä¸ªç‰¹å¾ (é™ç»´ 58%)

print(f"ä¿ç•™ç‰¹å¾æ•°: {len(high_var_features)}")
```

**ç­–ç•¥3: ä¿ç•™Top Ké«˜æ–¹å·®ç‰¹å¾**
```python
# ä¿ç•™å‰1000ä¸ªé«˜æ–¹å·®ç‰¹å¾
k = 1000
top_k_features = feature_vars.head(k).index
X_top_k = train_df[top_k_features]
# é™ç»´è‡³ 1000 ç‰¹å¾ (é™ç»´ 92%)
```

**æ¨è**: å…ˆä½¿ç”¨ç­–ç•¥1åˆ é™¤æä½æ–¹å·®ç‰¹å¾ï¼Œå†ç»“åˆç›¸å…³æ€§åˆ†æè¿›ä¸€æ­¥ç­›é€‰ã€‚

---

## ç‰¹å¾-æ ‡ç­¾ç›¸å…³æ€§åˆ†æ

### ç†è®ºåŸºç¡€ â­â­ æœ€æ ¸å¿ƒæ¦‚å¿µ

**ä»€ä¹ˆæ˜¯ç›¸å…³æ€§ï¼Ÿ**

Pearsonç›¸å…³ç³»æ•°è¡¡é‡ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ï¼š

```
r = Î£[(xi - xÌ„)(yi - È³)] / âˆš[Î£(xi - xÌ„)Â² Ã— Î£(yi - È³)Â²]
```

å…¶ä¸­:
- `xi`: ç‰¹å¾å€¼
- `yi`: æ ‡ç­¾å€¼ï¼ˆ0æˆ–1ï¼‰  
- `xÌ„, È³`: å‡å€¼
- `r`: ç›¸å…³ç³»æ•°ï¼ŒèŒƒå›´ [-1, 1]

**ç›¸å…³ç³»æ•°çš„å«ä¹‰**:
- **r = 1**: å®Œå…¨æ­£ç›¸å…³
- **r = -1**: å®Œå…¨è´Ÿç›¸å…³
- **r = 0**: æ— çº¿æ€§å…³ç³»
- **|r| > 0.3**: æœ‰å®é™…æ„ä¹‰çš„ç›¸å…³æ€§

**ä¸ºä»€ä¹ˆå–ç»å¯¹å€¼ï¼Ÿ**

```python
.abs()
```

- æ­£ç›¸å…³ï¼ˆr=0.5ï¼‰å’Œè´Ÿç›¸å…³ï¼ˆr=-0.5ï¼‰çš„é¢„æµ‹èƒ½åŠ›ç›¸åŒ
- æˆ‘ä»¬åªå…³å¿ƒç›¸å…³æ€§çš„**å¼ºåº¦**ï¼Œä¸å…³å¿ƒæ–¹å‘
- ç»å¯¹å€¼åæ’åºï¼Œæ‰¾å‡ºæœ€ç›¸å…³çš„ç‰¹å¾

### åˆ†æä»£ç 

```python
# è®¡ç®—æ¯ä¸ªç‰¹å¾ä¸æ ‡ç­¾çš„ç›¸å…³æ€§ï¼ˆå–ç»å¯¹å€¼ï¼‰
label_corr = train_df[feature_cols].corrwith(train_df[label_col]).abs()
label_corr = label_corr.sort_values(ascending=False)

print(f"\nTop 20 features most correlated with label:")
print(label_corr.head(20))

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(18, 6))

# ç›¸å…³æ€§åˆ†å¸ƒç›´æ–¹å›¾
axes[0].hist(label_corr.values, bins=50, color='teal', edgecolor='black', alpha=0.7)
axes[0].set_title('Distribution of Feature-Label Correlations', 
                   fontsize=13, fontweight='bold')
axes[0].set_xlabel('Absolute Correlation')
axes[0].set_ylabel('Number of Features')
axes[0].axvline(label_corr.median(), color='red', linestyle='--', 
                label=f'Median: {label_corr.median():.4f}')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Top 30 ç›¸å…³ç‰¹å¾
top_30_corr = label_corr.head(30)
axes[1].barh(range(len(top_30_corr)), top_30_corr.values, color='darkgreen')
axes[1].set_title('Top 30 Features by Correlation with Label', 
                   fontsize=13, fontweight='bold')
axes[1].set_xlabel('Absolute Correlation')
axes[1].set_ylabel('Feature Rank')
axes[1].invert_yaxis()
axes[1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig('../docs/images/03_feature_correlation.png', dpi=300, bbox_inches='tight')
plt.show()
```

### åˆ†æç»“æœ

![ç‰¹å¾ç›¸å…³æ€§](images/03_feature_correlation.png)

**ç»Ÿè®¡æ‘˜è¦**:

| ç›¸å…³æ€§èŒƒå›´ | ç‰¹å¾æ•°é‡ | å æ¯” | è¯„ä»· | é¢„æµ‹èƒ½åŠ› |
|------------|----------|------|------|----------|
| > 0.3ï¼ˆå¼ºç›¸å…³ï¼‰ | 4 | 0.03% | â­â­â­â­â­ | ä¼˜è´¨ç‰¹å¾ |
| 0.2-0.3ï¼ˆä¸­ç­‰ï¼‰ | 361 | 2.84% | â­â­â­â­ | è‰¯å¥½ç‰¹å¾ |
| 0.1-0.2ï¼ˆå¼±ç›¸å…³ï¼‰ | 3,292 | 25.92% | â­â­â­ | å¯ç”¨ç‰¹å¾ |
| â‰¤ 0.1ï¼ˆå‡ ä¹æ— å…³ï¼‰ | 9,043 | 71.20% | â­ | å™ªå£°ç‰¹å¾ |

**ä¸­ä½ç›¸å…³æ€§**: 0.0641ï¼ˆéå¸¸ä½ï¼‰

### ç›¸å…³æ€§èŒƒå›´è¯¦è§£

| èŒƒå›´ | è§£é‡Š | å®é™…æ„ä¹‰ |
|------|------|----------|
| **0.0 - 0.1** | å‡ ä¹æ— ç›¸å…³æ€§ | ç‰¹å¾ä¸æ ‡ç­¾å‡ ä¹æ²¡æœ‰çº¿æ€§å…³ç³» |
| **0.1 - 0.3** | å¼±ç›¸å…³ | æœ‰ä¸€å®šå…³ç³»ï¼Œä½†é¢„æµ‹èƒ½åŠ›æœ‰é™ |
| **0.3 - 0.5** | ä¸­ç­‰ç›¸å…³ | æœ‰æ˜æ˜¾å…³ç³»ï¼Œå€¼å¾—é‡ç‚¹å…³æ³¨ |
| **0.5 - 0.7** | å¼ºç›¸å…³ | å¾ˆå¼ºçš„é¢„æµ‹èƒ½åŠ› |
| **0.7 - 1.0** | éå¸¸å¼ºç›¸å…³ | æå¼ºçš„é¢„æµ‹èƒ½åŠ›ï¼ˆæœ¬æ•°æ®é›†ä¸­æ²¡æœ‰ï¼‰ |

### Top 20 æœ€ç›¸å…³ç‰¹å¾

| æ’å | ç‰¹å¾å | ç›¸å…³ç³»æ•° | ç­‰çº§ |
|------|--------|----------|------|
| 1 | f11394 | 0.3201 | â­â­â­â­â­ |
| 2 | f6044 | 0.3105 | â­â­â­â­â­ |
| 3 | f11105 | 0.3078 | â­â­â­â­â­ |
| 4 | f8339 | 0.3037 | â­â­â­â­â­ |
| 5 | f1432 | 0.2992 | â­â­â­â­ |
| 6 | f9618 | 0.2962 | â­â­â­â­ |
| 7 | f11613 | 0.2961 | â­â­â­â­ |
| 8 | f8628 | 0.2942 | â­â­â­â­ |
| 9 | f11451 | 0.2918 | â­â­â­â­ |
| 10 | f11488 | 0.2860 | â­â­â­â­ |
| 11 | f12528 | 0.2855 | â­â­â­â­ |
| 12 | f4018 | 0.2842 | â­â­â­â­ |
| 13 | f4010 | 0.2840 | â­â­â­â­ |
| 14 | f411 | 0.2821 | â­â­â­â­ |
| 15 | f10446 | 0.2820 | â­â­â­â­ |
| 16 | f921 | 0.2814 | â­â­â­â­ |
| 17 | f5883 | 0.2797 | â­â­â­â­ |
| 18 | f11983 | 0.2796 | â­â­â­â­ |
| 19 | f8436 | 0.2794 | â­â­â­â­ |
| 20 | f1179 | 0.2791 | â­â­â­â­ |

### æ·±å…¥è§£è¯»

**1. å·¦å›¾ï¼šç›¸å…³æ€§åˆ†å¸ƒ**

- **ä¸­ä½æ•°ä»…0.0641**: 
  - å¤§éƒ¨åˆ†ç‰¹å¾ä¸æ ‡ç­¾å‡ ä¹æ— å…³
  - æ•°æ®ä¸­å™ªå£°ç‰¹å¾å ä¸»å¯¼
  
- **åˆ†å¸ƒå½¢æ€**:
  - å·¦ååˆ†å¸ƒï¼Œé›†ä¸­åœ¨ä½ç›¸å…³æ€§åŒºåŸŸ
  - ç¬¦åˆé«˜ç»´æ•°æ®çš„å…¸å‹ç‰¹å¾
  
**2. å³å›¾ï¼šTop 30ç‰¹å¾**

- **æœ€é«˜ç›¸å…³æ€§ä»…0.32**:
  - ç›¸å¯¹è¾ƒä½ï¼Œè¯´æ˜å•ä¸ªç‰¹å¾é¢„æµ‹èƒ½åŠ›æœ‰é™
  - éœ€è¦ç»„åˆå¤šä¸ªç‰¹å¾æ‰èƒ½å–å¾—å¥½æ•ˆæœ
  
- **æ¢¯åº¦ä¸‹é™å¹³ç¼“**:
  - Top 30ç‰¹å¾çš„ç›¸å…³æ€§å·®å¼‚ä¸å¤§
  - è¯´æ˜æ²¡æœ‰"ç»å¯¹ä¼˜åŠ¿"çš„ç‰¹å¾

**3. å…³é”®å‘ç°**

1. ğŸ”´ **ç›¸å…³æ€§æ™®éå¾ˆä½**: 
   - 71.2%çš„ç‰¹å¾å‡ ä¹æ— å…³ï¼ˆâ‰¤0.1ï¼‰
   - è¿™æ˜¯é«˜ç»´å°æ ·æœ¬æ•°æ®çš„å…¸å‹ç‰¹å¾
   
2. âš ï¸ **ä¼˜è´¨ç‰¹å¾ç¨€ç¼º**: 
   - åªæœ‰4ä¸ªç‰¹å¾ç›¸å…³æ€§>0.3
   - ä»…365ä¸ªç‰¹å¾ç›¸å…³æ€§>0.2ï¼ˆ2.9%ï¼‰
   
3. ğŸ’¡ **ç‰¹å¾é€‰æ‹©å¿…è¦æ€§**:
   - å¤§éƒ¨åˆ†ç‰¹å¾æ˜¯å™ªå£°
   - å¿…é¡»è¿›è¡Œç‰¹å¾é€‰æ‹©ä»¥æå‡ä¿¡å™ªæ¯”

4. ğŸ“Š **éœ€è¦ç‰¹å¾ç»„åˆ**:
   - æœ€å¼ºç‰¹å¾ä»…0.32ï¼Œé¢„æµ‹èƒ½åŠ›æœ‰é™
   - éœ€è¦é€šè¿‡æ¨¡å‹å­¦ä¹ ç‰¹å¾ç»„åˆ
   - é›†æˆå­¦ä¹ å¯èƒ½æ•ˆæœæ›´å¥½

### ç‰¹å¾é€‰æ‹©ç­–ç•¥

**ç­–ç•¥1: é€‰æ‹©Top Kä¸ªç‰¹å¾ï¼ˆç®€å•ç›´æ¥ï¼‰**
```python
# é€‰æ‹©ç›¸å…³æ€§æœ€é«˜çš„100ä¸ªç‰¹å¾
top_k = 100  # æˆ– 200, 500
selected_features = label_corr.nlargest(top_k).index
X_selected = train_df[selected_features]

print(f"é€‰æ‹©äº† {len(selected_features)} ä¸ªç‰¹å¾")
print(f"é™ç»´æ¯”ä¾‹: {(1 - top_k/12700)*100:.1f}%")
```

**ç­–ç•¥2: æŒ‰é˜ˆå€¼ç­›é€‰ï¼ˆåŸºäºä¸šåŠ¡éœ€æ±‚ï¼‰**
```python
# é€‰æ‹©ç›¸å…³æ€§ > 0.15 çš„ç‰¹å¾
threshold = 0.15
selected_features = label_corr[label_corr > threshold].index
X_selected = train_df[selected_features]
# é¢„è®¡å¯é€‰å‡ºçº¦ 1,500-2,000 ä¸ªç‰¹å¾

print(f"ç›¸å…³æ€§ > {threshold} çš„ç‰¹å¾æ•°: {len(selected_features)}")
```

**ç­–ç•¥3: ç»„åˆæ–¹å·®å’Œç›¸å…³æ€§ï¼ˆæ¨èï¼‰â­â­â­â­â­**
```python
# åŒæ—¶æ»¡è¶³ï¼šæ–¹å·® >= 0.1 ä¸” ç›¸å…³æ€§ >= 0.15
high_var = feature_vars >= 0.1
high_corr = label_corr >= 0.15
selected = high_var & high_corr
selected_features = train_df.columns[selected]

print(f"åŒæ—¶æ»¡è¶³é«˜æ–¹å·®å’Œé«˜ç›¸å…³æ€§çš„ç‰¹å¾: {len(selected_features)}")
```

**ç­–ç•¥4: åˆ†å±‚é€‰æ‹©ï¼ˆè¿›é˜¶ï¼‰**
```python
# ä¼˜è´¨ç‰¹å¾ï¼ˆç›¸å…³æ€§ > 0.2ï¼‰å…¨éƒ¨ä¿ç•™
tier1 = label_corr[label_corr > 0.2].index  # 365ä¸ª

# è‰¯å¥½ç‰¹å¾ï¼ˆ0.15 < ç›¸å…³æ€§ â‰¤ 0.2ï¼‰é€‰æ‹©æ–¹å·®æœ€é«˜çš„500ä¸ª
tier2_candidates = label_corr[(label_corr > 0.15) & (label_corr <= 0.2)].index
tier2_vars = feature_vars[tier2_candidates].nlargest(500).index

# åˆå¹¶
selected_features = tier1.union(tier2_vars)
# æ€»å…±çº¦ 865 ä¸ªç‰¹å¾

print(f"åˆ†å±‚é€‰æ‹©ç‰¹å¾æ•°: {len(selected_features)}")
```

### å®æˆ˜åº”ç”¨ç¤ºä¾‹

```python
# å®Œæ•´çš„ç‰¹å¾é€‰æ‹©æµç¨‹
from sklearn.feature_selection import SelectKBest, f_classif

# æ­¥éª¤1: åŸºäºæ–¹å·®åˆç­›ï¼ˆåˆ é™¤ä½æ–¹å·®ç‰¹å¾ï¼‰
from sklearn.feature_selection import VarianceThreshold
var_selector = VarianceThreshold(threshold=0.01)
X_step1 = var_selector.fit_transform(X)  # 12700 -> 12169

# æ­¥éª¤2: åŸºäºç›¸å…³æ€§é€‰æ‹©Top K
k = 500
corr_selector = SelectKBest(f_classif, k=k)
X_selected = corr_selector.fit_transform(X_step1, y)  # 12169 -> 500

print(f"æœ€ç»ˆä¿ç•™ç‰¹å¾æ•°: {X_selected.shape[1]}")
print(f"æ€»é™ç»´æ¯”ä¾‹: {(1 - X_selected.shape[1]/12700)*100:.1f}%")

# è·å–è¢«é€‰ä¸­çš„ç‰¹å¾å
selected_mask = corr_selector.get_support()
selected_features = feature_cols[var_selector.get_support()][selected_mask]
print(f"\nè¢«é€‰ä¸­çš„ç‰¹å¾:")
print(selected_features[:20])  # æ˜¾ç¤ºå‰20ä¸ª
```

---

## Topç‰¹å¾è¯¦ç»†åˆ†æ

### åˆ†æç›®çš„

é€šè¿‡å¯è§†åŒ–Topç‰¹å¾åœ¨ä¸åŒç±»åˆ«ä¸­çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥ï¼š
1. **éªŒè¯ç›¸å…³æ€§åˆ†æ**: ç›¸å…³æ€§é«˜çš„ç‰¹å¾æ˜¯å¦çœŸçš„æœ‰åŒºåˆ†èƒ½åŠ›ï¼Ÿ
2. **ç†è§£ç‰¹å¾ç‰¹æ€§**: åˆ†å¸ƒå½¢æ€ã€å¼‚å¸¸å€¼ã€ç±»åˆ«å·®å¼‚
3. **æŒ‡å¯¼ç‰¹å¾å·¥ç¨‹**: å‘ç°å¯èƒ½çš„ç‰¹å¾å˜æ¢æœºä¼š

### ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”

### å¯è§†åŒ–ä»£ç 

```python
# é€‰æ‹©Top 9ä¸ªæœ€ç›¸å…³çš„ç‰¹å¾
top_features = label_corr.head(9).index

# åˆ›å»º3x3å­å›¾
fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 12))
axes = axes.ravel()

for i, feature in enumerate(top_features):
    # ç»˜åˆ¶æŒ‰ç±»åˆ«åˆ†ç»„çš„ç›´æ–¹å›¾ + KDEæ›²çº¿
    sns.histplot(data=train_df, x=feature, hue=label_col, kde=True, 
                 ax=axes[i], element='step', palette='Set1', 
                 alpha=0.6, stat='density')
    axes[i].set_title(f'{feature}\n(Corr: {label_corr[feature]:.4f})', 
                     fontsize=10, fontweight='bold')
    axes[i].legend(title='Class', labels=['0', '1'], fontsize=8)
    axes[i].grid(True, alpha=0.3)

plt.suptitle('Top 9 Features Most Correlated with Label', 
             fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig('../docs/images/04_top_features_distribution.png', dpi=300, bbox_inches='tight')
plt.show()
```

**KDEï¼ˆæ ¸å¯†åº¦ä¼°è®¡ï¼‰çš„ä½œç”¨**:
- æä¾›å¹³æ»‘çš„æ¦‚ç‡å¯†åº¦æ›²çº¿
- æ¯”ç›´æ–¹å›¾æ›´å®¹æ˜“çœ‹å‡ºåˆ†å¸ƒå½¢æ€
- å¸®åŠ©åˆ¤æ–­åˆ†å¸ƒæ˜¯å¦ä¸ºæ­£æ€ã€åŒå³°ç­‰

### åˆ†æç»“æœ

![Topç‰¹å¾åˆ†å¸ƒ](images/04_top_features_distribution.png)

**é€ä¸ªç‰¹å¾åˆ†æ**:

#### 1. f11394 (ç›¸å…³æ€§: 0.3201) - æœ€ä½³ç‰¹å¾ â­â­â­â­â­

- **åˆ†å¸ƒç‰¹å¾**:
  - ç±»åˆ«0: åˆ†å¸ƒä¸­å¿ƒåœ¨è´Ÿå€¼åŒºåŸŸï¼ˆçº¦-0.4ï¼‰
  - ç±»åˆ«1: åˆ†å¸ƒæ›´åˆ†æ•£ï¼Œä¸­å¿ƒåå³
  - ä¸¤ç±»åˆ†å¸ƒæœ‰æ˜æ˜¾åˆ†ç¦»
  
- **åŒºåˆ†èƒ½åŠ›**: â­â­â­â­â­ (ä¼˜ç§€)
- **å»ºè®®**: æ ¸å¿ƒç‰¹å¾ï¼Œå¿…é¡»ä¿ç•™

#### 2. f6044 (ç›¸å…³æ€§: 0.3105) â­â­â­â­â­

- **åˆ†å¸ƒç‰¹å¾**:
  - ç±»åˆ«0: é›†ä¸­åœ¨0.5-1.0åŒºé—´
  - ç±»åˆ«1: é›†ä¸­åœ¨1.0-1.5åŒºé—´
  - åˆ†å¸ƒé‡å é€‚ä¸­ï¼Œæœ‰æ¸…æ™°çš„å³ç§»
  
- **åŒºåˆ†èƒ½åŠ›**: â­â­â­â­â­ (ä¼˜ç§€)
- **ç‰¹ç‚¹**: ä½ç½®åç§»å‹ç‰¹å¾

#### 3. f11105 (ç›¸å…³æ€§: 0.3078) â­â­â­â­â­

- **åˆ†å¸ƒç‰¹å¾**:
  - å‘ˆç°æ˜æ˜¾çš„åŒå³°åˆ†å¸ƒ
  - ä¸¤ç±»è™½æœ‰é‡å ä½†å³°å€¼ä½ç½®ä¸åŒ
  - ç±»åˆ«1çš„åˆ†å¸ƒæ›´åå³
  
- **åŒºåˆ†èƒ½åŠ›**: â­â­â­â­ (ä¼˜ç§€)
- **ç‰¹ç‚¹**: åŒå³°åˆ†å¸ƒç‰¹å¾ï¼Œå¯èƒ½éœ€è¦éçº¿æ€§æ¨¡å‹

#### 4. f8339 (ç›¸å…³æ€§: 0.3037) â­â­â­â­

- **åˆ†å¸ƒç‰¹å¾**:
  - ç±»åˆ«0: å•å³°ï¼Œé›†ä¸­åœ¨0.4-0.6
  - ç±»åˆ«1: åˆ†æ•£ï¼Œåå‘è¾ƒä½å€¼
  - æ–¹å·®å·®å¼‚æ˜æ˜¾
  
- **åŒºåˆ†èƒ½åŠ›**: â­â­â­â­ (è‰¯å¥½)

#### 5. f1432 (ç›¸å…³æ€§: 0.2992) â­â­â­â­

- **åˆ†å¸ƒç‰¹å¾**:
  - ç±»åˆ«0: åˆ†å¸ƒè¾ƒçª„
  - ç±»åˆ«1: åˆ†å¸ƒè¾ƒå®½ï¼Œåå‘ä½å€¼
  - ä¸»è¦é€šè¿‡åˆ†å¸ƒå®½åº¦åŒºåˆ†
  
- **åŒºåˆ†èƒ½åŠ›**: â­â­â­ (ä¸­ç­‰)

#### 6-9. å…¶ä»–Topç‰¹å¾

å‰©ä½™ç‰¹å¾ä¹Ÿå±•ç°å‡ºä¸åŒç¨‹åº¦çš„ç±»åˆ«åˆ†ç¦»ï¼Œä½†ï¼š
- é‡å åŒºåŸŸç›¸å¯¹è¾ƒå¤§
- å•ç‹¬ä½¿ç”¨åŒºåˆ†èƒ½åŠ›æœ‰é™
- éœ€è¦ç»„åˆä½¿ç”¨

### é€šç”¨è§‚å¯Ÿç»“è®º

**ä¼˜ç‚¹** âœ…:
1. **æœ‰æ•ˆåˆ†ç¦»**: æ‰€æœ‰Topç‰¹å¾éƒ½å±•ç°å‡ºå¯è§çš„ç±»åˆ«å·®å¼‚
2. **ç¬¦åˆé¢„æœŸ**: ç›¸å…³æ€§ä¸å®é™…åŒºåˆ†èƒ½åŠ›ä¸€è‡´
3. **äº’è¡¥æ€§**: ä¸åŒç‰¹å¾çš„åŒºåˆ†æ–¹å¼ä¸åŒï¼ˆä½ç½®ã€å½¢çŠ¶ã€æ–¹å·®ï¼‰

**å±€é™æ€§** âš ï¸:
1. **ä»æœ‰é‡å **: å³ä½¿æœ€å¥½çš„ç‰¹å¾ä¹Ÿå­˜åœ¨é‡å åŒºåŸŸ
2. **å•ç‰¹å¾ä¸è¶³**: ä»»ä½•å•ä¸ªç‰¹å¾éƒ½æ— æ³•å®Œç¾åˆ†ç±»
3. **éœ€è¦æ¨¡å‹**: å¿…é¡»é€šè¿‡æœºå™¨å­¦ä¹ æ¨¡å‹ç»„åˆç‰¹å¾

### ç®±çº¿å›¾åˆ†æ

```python
# åˆ›å»ºç®±çº¿å›¾
fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 12))
axes = axes.ravel()

for i, feature in enumerate(top_features):
    sns.boxplot(data=train_df, x=label_col, y=feature, 
                ax=axes[i], palette='Set2')
    axes[i].set_title(f'{feature}', fontsize=10, fontweight='bold')
    axes[i].set_xlabel('Class')
    axes[i].grid(True, alpha=0.3)

plt.suptitle('Box Plots of Top 9 Features (Class Comparison)', 
             fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig('../docs/images/05_top_features_boxplot.png', dpi=300, bbox_inches='tight')
plt.show()
```

### ç®±çº¿å›¾è§£è¯»åŸºç¡€

![Topç‰¹å¾ç®±çº¿å›¾](images/05_top_features_boxplot.png)

**ç®±çº¿å›¾çš„5ä¸ªå…³é”®å…ƒç´ **:

1. **ç®±ä½“ï¼ˆBoxï¼‰**: 
   - ä¸‹è¾¹ç•Œ = 25%åˆ†ä½æ•°ï¼ˆQ1ï¼‰
   - ä¸Šè¾¹ç•Œ = 75%åˆ†ä½æ•°ï¼ˆQ3ï¼‰
   - ç®±ä½“é«˜åº¦ = å››åˆ†ä½è·ï¼ˆIQR = Q3 - Q1ï¼‰
   
2. **ä¸­çº¿ï¼ˆMedian Lineï¼‰**: 
   - 50%åˆ†ä½æ•°ï¼ˆä¸­ä½æ•°ï¼‰
   - æ•°æ®çš„ä¸­å¿ƒä½ç½®
   
3. **é¡»ï¼ˆWhiskersï¼‰**: 
   - ä¸Šé¡» = min(æœ€å¤§å€¼, Q3 + 1.5Ã—IQR)
   - ä¸‹é¡» = max(æœ€å°å€¼, Q1 - 1.5Ã—IQR)
   - è¡¨ç¤ºæ­£å¸¸æ•°æ®èŒƒå›´
   
4. **ç¦»ç¾¤å€¼ï¼ˆOutliersï¼‰**: 
   - è¶…å‡ºé¡»èŒƒå›´çš„ç‚¹
   - å¯èƒ½æ˜¯å¼‚å¸¸å€¼æˆ–æç«¯å€¼
   
5. **ç±»åˆ«å¯¹æ¯”**: 
   - ä¸¤ä¸ªç®±ä½“çš„ç›¸å¯¹ä½ç½®
   - é‡å ç¨‹åº¦åæ˜ åŒºåˆ†éš¾åº¦

### å…³é”®è§‚å¯Ÿ

**1. ä¸­ä½æ•°å·®å¼‚åˆ†æ**

- âœ… **æ‰€æœ‰Top 9ç‰¹å¾**çš„ä¸¤ç±»ä¸­ä½æ•°éƒ½æœ‰æ˜æ˜¾å·®å¼‚
- âœ… éªŒè¯äº†è¿™äº›ç‰¹å¾çš„**åˆ¤åˆ«èƒ½åŠ›**
- ğŸ“Š å·®å¼‚ç¨‹åº¦ä¸ç›¸å…³ç³»æ•°åŸºæœ¬ä¸€è‡´

**2. ç¦»ç¾¤å€¼æ£€æµ‹**

| ç‰¹å¾ | ç±»åˆ«0ç¦»ç¾¤å€¼ | ç±»åˆ«1ç¦»ç¾¤å€¼ | è¯„ä¼° |
|------|-------------|-------------|------|
| f11394 | å°‘é‡ | å°‘é‡ | âœ… æ­£å¸¸ |
| f6044 | 1ä¸ª | 1ä¸ª | âœ… æ­£å¸¸ |
| f8339 | æ•°ä¸ª | 1ä¸ª | âœ… å¯æ¥å— |
| f1432 | æ—  | 1ä¸ª | âœ… ä¼˜ç§€ |

- **ç»“è®º**: ç¦»ç¾¤å€¼æ•°é‡é€‚ä¸­ï¼ˆ<5%ï¼‰ï¼Œä¸éœ€è¦ç‰¹åˆ«å¤„ç†
- **åŸå› **: è¿™äº›ç¦»ç¾¤å€¼å¯èƒ½åŒ…å«é‡è¦ä¿¡æ¯ï¼Œä¸å»ºè®®ç›´æ¥åˆ é™¤
- **å»ºè®®**: ä¿ç•™ç¦»ç¾¤å€¼ï¼Œè®©æ¨¡å‹è‡ªè¡Œå­¦ä¹ 

**3. åˆ†å¸ƒå½¢çŠ¶å¯¹æ¯”**

- **f11394, f8339**: ç±»åˆ«0çš„ç®±ä½“è¾ƒå¤§ â†’ æ–¹å·®æ›´å¤§ â†’ åˆ†å¸ƒæ›´åˆ†æ•£
- **f6044**: ä¸¤ç±»ç®±ä½“å¤§å°ç›¸è¿‘ï¼Œä½†**ä½ç½®ä¸åŒ** â†’ ä½ç½®åç§»å‹
- **f1432**: ç±»åˆ«1ç®±ä½“æ›´å¤§ä¸”æœ‰ç¦»ç¾¤å€¼ â†’ åˆ†å¸ƒæ›´å¤æ‚
- **f11451, f8628**: ä¸¤ç±»ç®±ä½“å‡ ä¹æ— é‡å  â†’ **ä¼˜ç§€çš„åŒºåˆ†èƒ½åŠ›**

### æ•°æ®è´¨é‡ç»“è®º

âœ… **æ•°æ®è´¨é‡è‰¯å¥½**:
- æ— éœ€å¤§è§„æ¨¡æ•°æ®æ¸…æ´—
- ç¦»ç¾¤å€¼åœ¨åˆç†èŒƒå›´å†…
- Topç‰¹å¾ç¡®å®å…·æœ‰åŒºåˆ†èƒ½åŠ›

âœ… **ç‰¹å¾æœ‰æ•ˆæ€§ç¡®è®¤**:
- ç›¸å…³æ€§åˆ†æçš„ç»“æœå¾—åˆ°éªŒè¯
- å¯ä»¥æ”¾å¿ƒä½¿ç”¨è¿™äº›ç‰¹å¾å»ºæ¨¡

âš ï¸ **æ³¨æ„äº‹é¡¹**:
- å•ä¸ªç‰¹å¾åŒºåˆ†èƒ½åŠ›æœ‰é™
- éœ€è¦å¤šç‰¹å¾ç»„åˆ
- å»ºè®®ä½¿ç”¨é›†æˆæ–¹æ³•

---

## 6. PCAé™ç»´åˆ†æ

### 6.1 ä¸»æˆåˆ†è§£é‡Šæ–¹å·®

![PCAåˆ†æ](images/06_pca_analysis.png)

**å…³é”®æŒ‡æ ‡**:

- **å‰10ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®**: 34.48%
- **å‰50ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®**: 59.68%

**åˆ†æä¸‰ä¸ªå­å›¾**:

#### 6.1.1 ç¢çŸ³å›¾ï¼ˆScree Plotï¼‰

**è§‚å¯Ÿ**:
- ç¬¬1ä¸ªä¸»æˆåˆ†è§£é‡Šäº†çº¦7.6%çš„æ–¹å·®ï¼ˆæœ€é«˜ï¼‰
- ç¬¬2ä¸ªä¸»æˆåˆ†è§£é‡Šäº†çº¦6.5%çš„æ–¹å·®
- ä»ç¬¬3ä¸ªä¸»æˆåˆ†å¼€å§‹ï¼Œè§£é‡Šæ–¹å·®å¿«é€Ÿä¸‹é™
- ç¬¬10ä¸ªä¸»æˆåˆ†åè¶‹äºå¹³ç¼“

**"è‚˜éƒ¨"ä½ç½®**: 
- å¤§çº¦åœ¨ç¬¬8-12ä¸ªä¸»æˆåˆ†é™„è¿‘
- è¿™æ„å‘³ç€å‰10-15ä¸ªä¸»æˆåˆ†åŒ…å«äº†ä¸»è¦ä¿¡æ¯

#### 6.1.2 ç´¯ç§¯è§£é‡Šæ–¹å·®å›¾

**å…³é”®ç‚¹**:

| ä¸»æˆåˆ†æ•° | ç´¯ç§¯è§£é‡Šæ–¹å·® | è¯´æ˜ |
|----------|--------------|------|
| 10 | 34.48% | éœ€è¦æ›´å¤š |
| 20 | 45-50% | ä»ä¸å……åˆ† |
| 50 | 59.68% | å‹‰å¼ºæ¥å— |
| 80 | ~70-75% | è¾ƒå¥½ |
| 100+ | >80% | æ¨è |

**ç»“è®º**:
- âŒ **ä¸è¾¾æ ‡**: 50ä¸ªä¸»æˆåˆ†ä»…è§£é‡Š60%çš„æ–¹å·®ï¼Œä¿¡æ¯æŸå¤±è¾ƒå¤§
- âš ï¸ **é«˜ç»´ç‰¹æ€§**: æ•°æ®çš„å†…åœ¨ç»´åº¦å¾ˆé«˜ï¼Œå¾ˆéš¾ç”¨å°‘æ•°ä¸»æˆåˆ†è¡¨ç¤º
- ğŸ’¡ **å»ºè®®**: å¦‚ä½¿ç”¨PCAï¼Œè‡³å°‘ä¿ç•™100-200ä¸ªä¸»æˆåˆ†ä»¥ä¿æŒ80-90%çš„æ–¹å·®

#### 6.1.3 å‰ä¸¤ä¸ªä¸»æˆåˆ†æ•£ç‚¹å›¾

**è§†è§‰è§‚å¯Ÿ**:

- **é¢œè‰²ç¼–ç **: è“è‰²=ç±»åˆ«0ï¼Œçº¢è‰²=ç±»åˆ«1
- **åˆ†å¸ƒæƒ…å†µ**: 
  - ä¸¤ç±»æœ‰ä¸€å®šç¨‹åº¦çš„åˆ†ç¦»
  - ä½†ä¹Ÿæœ‰è¾ƒå¤§çš„é‡å åŒºåŸŸ
  - PC1æ–¹å‘ä¸Šçš„åˆ†ç¦»ä¼˜äºPC2

**å¯åˆ†æ€§è¯„ä¼°**: â­â­â­ (ä¸­ç­‰)

**è§£é‡Š**:
- å‰ä¸¤ä¸ªä¸»æˆåˆ†ä»…è§£é‡Š14%çš„æ–¹å·®ï¼Œå¤§é‡ä¿¡æ¯è¢«ä¸¢å¼ƒ
- çœŸå®çš„ç±»åˆ«åˆ†ç¦»å¯èƒ½å­˜åœ¨äºé«˜ç»´ç©ºé—´ä¸­
- è¿™è¿›ä¸€æ­¥è¯´æ˜äº†è¿™æ˜¯ä¸€ä¸ª**é«˜ç»´å¤æ‚é—®é¢˜**

---

## 7. ç»¼åˆæ•°æ®æ´å¯Ÿ

### 7.1 æ•°æ®æŒ‘æˆ˜æ€»ç»“

| æŒ‘æˆ˜ | ä¸¥é‡ç¨‹åº¦ | å½±å“ |
|------|----------|------|
| **é«˜ç»´åº¦** (12,700ç‰¹å¾) | ğŸ”´ æé«˜ | ç»´åº¦è¯…å’’ã€è¿‡æ‹Ÿåˆé£é™© |
| **å°æ ·æœ¬** (196ä¸ª) | ğŸ”´ æé«˜ | ç»Ÿè®¡ä¸ç¨³å®šã€æ³›åŒ–å›°éš¾ |
| **ç‰¹å¾/æ ·æœ¬æ¯”** (64.8:1) | ğŸ”´ æé«˜ | æ¨¡å‹é€‰æ‹©å—é™ |
| **ä½ç›¸å…³æ€§** (ä¸­ä½0.064) | ğŸŸ¡ ä¸­ç­‰ | å•ç‰¹å¾é¢„æµ‹èƒ½åŠ›å¼± |
| **ç±»åˆ«ä¸å¹³è¡¡** (64.3% vs 35.7%) | ğŸŸ¢ è½»å¾® | å¯é€šè¿‡ç®€å•ç­–ç•¥ç¼“è§£ |
| **æ•°æ®è´¨é‡** | ğŸŸ¢ è‰¯å¥½ | æ— ç¼ºå¤±ã€æ— å¸¸é‡ç‰¹å¾ |

### 7.2 å…³é”®ç»Ÿè®¡æ•°æ®

**ç‰¹å¾è´¨é‡åˆ†å±‚**:

```
ä¼˜è´¨ç‰¹å¾ (ç›¸å…³æ€§ > 0.3):        4 ä¸ª    (0.03%)  â­â­â­â­â­
è‰¯å¥½ç‰¹å¾ (0.2 < ç›¸å…³æ€§ â‰¤ 0.3):  361 ä¸ª   (2.84%)  â­â­â­â­
å¯ç”¨ç‰¹å¾ (0.1 < ç›¸å…³æ€§ â‰¤ 0.2):  3,292 ä¸ª (25.92%) â­â­â­
å¼±ç‰¹å¾ (ç›¸å…³æ€§ â‰¤ 0.1):          9,043 ä¸ª (71.20%) â­â­
```

**æ–¹å·®è´¨é‡åˆ†å±‚**:

```
é«˜æ–¹å·®ç‰¹å¾ (æ–¹å·® â‰¥ 1.0):       ~500 ä¸ª   â­â­â­â­
ä¸­ç­‰æ–¹å·® (0.1 â‰¤ æ–¹å·® < 1.0):   ~4,800 ä¸ª â­â­â­  
ä½æ–¹å·® (0.01 â‰¤ æ–¹å·® < 0.1):    ~6,900 ä¸ª â­â­
æä½æ–¹å·® (æ–¹å·® < 0.01):         531 ä¸ª    â­
```

---

## 8. å»ºæ¨¡ç­–ç•¥å»ºè®®

### 8.1 ç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼ˆé‡è¦ï¼ï¼‰

**ç­–ç•¥ç»„åˆæ–¹æ¡ˆ**:

#### æ–¹æ¡ˆ1: ä¿å®ˆç­–ç•¥ï¼ˆæ¨èåˆå­¦è€…ï¼‰
```python
# æ­¥éª¤1: åˆ é™¤æä½æ–¹å·®ç‰¹å¾
selector_var = VarianceThreshold(threshold=0.01)
X_step1 = selector_var.fit_transform(X)  # å‰©ä½™ ~12,170 ç‰¹å¾

# æ­¥éª¤2: é€‰æ‹©Topç›¸å…³æ€§ç‰¹å¾
k = 500
selector_corr = SelectKBest(f_classif, k=k)
X_selected = selector_corr.fit_transform(X_step1, y)  # å‰©ä½™ 500 ç‰¹å¾
```

**é¢„æœŸæ•ˆæœ**: ç‰¹å¾ä»12,700é™åˆ°500ï¼Œé™ç»´æ¯”ä¾‹96%

#### æ–¹æ¡ˆ2: PCAé™ç»´ï¼ˆæ¨èé«˜ç»´æ•°æ®ï¼‰
```python
# æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCAä¿ç•™90%æ–¹å·®
pca = PCA(n_components=0.90)
X_pca = pca.fit_transform(X_scaled)  # é¢„è®¡ ~100-150 ä¸ªä¸»æˆåˆ†

print(f"é™ç»´åˆ° {X_pca.shape[1]} ç»´")
```

**é¢„æœŸæ•ˆæœ**: é™è‡³100-150ç»´ï¼Œä¿¡æ¯ä¿ç•™90%

#### æ–¹æ¡ˆ3: é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆæ¨èè¿›é˜¶ï¼‰
```python
from sklearn.feature_selection import RFECV
from sklearn.linear_model import LogisticRegression

# å…ˆç”¨æ–¹æ¡ˆ1é™åˆ°500ç»´
# ç„¶åä½¿ç”¨RFECVè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç‰¹å¾æ•°
estimator = LogisticRegression(penalty='l2', C=1.0, max_iter=1000)
selector = RFECV(estimator, step=50, cv=5, scoring='roc_auc')
X_optimal = selector.fit_transform(X_step2, y)

print(f"æœ€ä¼˜ç‰¹å¾æ•°: {selector.n_features_}")
```

**é¢„æœŸæ•ˆæœ**: è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜ç‰¹å¾å­é›†

#### æ–¹æ¡ˆ4: ç»„åˆç­–ç•¥ï¼ˆæ¨èä¸“å®¶ï¼‰
```python
# æ­¥éª¤1: æ–¹å·®ç­›é€‰ (12700 -> 12170)
X1 = variance_filter(X, threshold=0.01)

# æ­¥éª¤2: ç›¸å…³æ€§ç­›é€‰ (12170 -> 1000)  
X2 = correlation_filter(X1, y, threshold=0.1)

# æ­¥éª¤3: äº’ä¿¡æ¯ç­›é€‰ (1000 -> 500)
from sklearn.feature_selection import mutual_info_classif
X3 = mutual_info_filter(X2, y, k=500)

# æ­¥éª¤4: L1æ­£åˆ™åŒ– (500 -> æœ€ä¼˜)
from sklearn.linear_model import LassoCV
X_final = lasso_selection(X3, y)
```

### 8.2 æ¨èæ¨¡å‹

**é€‚åˆé«˜ç»´å°æ ·æœ¬çš„æ¨¡å‹ï¼ˆæŒ‰æ¨èåº¦æ’åºï¼‰**:

#### 1. å¸¦æ­£åˆ™åŒ–çš„é€»è¾‘å›å½’ â­â­â­â­â­
```python
from sklearn.linear_model import LogisticRegressionCV

model = LogisticRegressionCV(
    penalty='elasticnet',  # L1+L2ç»„åˆ
    solver='saga',
    cv=5,
    l1_ratios=[0.1, 0.5, 0.9],
    max_iter=10000,
    class_weight='balanced'
)
```

**ä¼˜ç‚¹**:
- âœ… å†…ç½®æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
- âœ… å¯è§£é‡Šæ€§å¼º
- âœ… è®­ç»ƒé€Ÿåº¦å¿«
- âœ… é€‚åˆé«˜ç»´æ•°æ®

#### 2. æ”¯æŒå‘é‡æœº (SVM) â­â­â­â­â­
```python
from sklearn.svm import SVC

model = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',
    class_weight='balanced',
    probability=True
)
```

**ä¼˜ç‚¹**:
- âœ… é€‚åˆå°æ ·æœ¬
- âœ… é«˜ç»´ç©ºé—´è¡¨ç°å¥½
- âœ… æ ¸å‡½æ•°çµæ´»

#### 3. éšæœºæ£®æ— â­â­â­â­
```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=500,
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=5,
    class_weight='balanced',
    max_features='sqrt'
)
```

**ä¼˜ç‚¹**:
- âœ… æä¾›ç‰¹å¾é‡è¦æ€§
- âœ… å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ
- âœ… ä¸éœ€è¦ç‰¹å¾æ ‡å‡†åŒ–

âš ï¸ **æ³¨æ„**: éœ€è¦é™åˆ¶æ ‘æ·±åº¦é˜²æ­¢è¿‡æ‹Ÿåˆ

#### 4. XGBoost/LightGBM â­â­â­â­
```python
import xgboost as xgb

model = xgb.XGBClassifier(
    max_depth=5,
    learning_rate=0.05,
    n_estimators=500,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,  # L1æ­£åˆ™åŒ–
    reg_lambda=1.0,  # L2æ­£åˆ™åŒ–
    scale_pos_weight=1.8  # å¤„ç†ä¸å¹³è¡¡
)
```

**ä¼˜ç‚¹**:
- âœ… æ€§èƒ½å¼ºå¤§
- âœ… ç‰¹å¾é€‰æ‹©è‡ªåŠ¨åŒ–
- âœ… å¤„ç†å¤æ‚æ¨¡å¼

#### 5. ç¥ç»ç½‘ç»œ (MLP) â­â­â­â­
```python
from sklearn.neural_network import MLPClassifier

model = MLPClassifier(
    hidden_layer_sizes=(256, 128, 64),  # ä¸‰å±‚ç½‘ç»œ
    activation='relu',
    solver='adam',
    alpha=0.001,  # L2æ­£åˆ™åŒ–
    learning_rate_init=0.001,
    max_iter=500,
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=20,  # æ—©åœè€å¿ƒå€¼
    random_state=42
)
```

**ä¼˜ç‚¹**:
- âœ… å¯å­¦ä¹ å¤æ‚éçº¿æ€§å…³ç³»
- âœ… é€‚åˆé«˜ç»´æ•°æ®
- âœ… è‡ªåŠ¨ç‰¹å¾ç»„åˆ
- âœ… æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ

**æ¶æ„è¯´æ˜**:
- **è¾“å…¥å±‚**: 500ç»´ (ç‰¹å¾é€‰æ‹©å)
- **éšè—å±‚1**: 256ä¸ªç¥ç»å…ƒ (ReLUæ¿€æ´»)
- **éšè—å±‚2**: 128ä¸ªç¥ç»å…ƒ (ReLUæ¿€æ´»)  
- **éšè—å±‚3**: 64ä¸ªç¥ç»å…ƒ (ReLUæ¿€æ´»)
- **è¾“å‡ºå±‚**: 2ä¸ªç¥ç»å…ƒ (Softmax)

âš ï¸ **æ³¨æ„**: éœ€è¦ç‰¹å¾æ ‡å‡†åŒ–ï¼Œè®­ç»ƒæ—¶é—´è¾ƒé•¿

### 8.3 äº¤å‰éªŒè¯ç­–ç•¥

**å¿…é¡»ä½¿ç”¨åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯**:

```python
from sklearn.model_selection import StratifiedKFold, cross_validate

# 5æŠ˜äº¤å‰éªŒè¯
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# å¤šæŒ‡æ ‡è¯„ä¼°
scoring = {
    'accuracy': 'accuracy',
    'roc_auc': 'roc_auc',
    'f1': 'f1',
    'precision': 'precision',
    'recall': 'recall'
}

results = cross_validate(
    model, X, y, 
    cv=skf, 
    scoring=scoring,
    return_train_score=True
)

print(f"ROC-AUC: {results['test_roc_auc'].mean():.4f} Â± {results['test_roc_auc'].std():.4f}")
print(f"Accuracy: {results['test_accuracy'].mean():.4f} Â± {results['test_accuracy'].std():.4f}")
```

**ä¸ºä»€ä¹ˆå¿…é¡»ç”¨5æŠ˜CVï¼Ÿ**
- âœ… æ ·æœ¬åªæœ‰196ä¸ªï¼Œå•æ¬¡åˆ’åˆ†ä¸å¯é 
- âœ… 5æŠ˜ç¡®ä¿æ¯æ¬¡æµ‹è¯•38-39ä¸ªæ ·æœ¬
- âœ… åˆ†å±‚ä¿è¯æ¯æŠ˜çš„ç±»åˆ«æ¯”ä¾‹ä¸€è‡´
- âœ… å¾—åˆ°ç¨³å®šçš„æ€§èƒ½ä¼°è®¡

### 8.4 è¯„ä¼°æŒ‡æ ‡é€‰æ‹©

**ä¸»è¦æŒ‡æ ‡**: ROC-AUCï¼ˆæ¨èï¼‰ â­â­â­â­â­

```python
from sklearn.metrics import roc_auc_score, roc_curve

roc_auc = roc_auc_score(y_true, y_pred_proba)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
```

**ä¸ºä»€ä¹ˆé€‰ROC-AUCï¼Ÿ**
- âœ… å¯¹ç±»åˆ«ä¸å¹³è¡¡ä¸æ•æ„Ÿ
- âœ… è¯„ä¼°æ•´ä½“æ’åºèƒ½åŠ›
- âœ… ä¸ä¾èµ–é˜ˆå€¼é€‰æ‹©
- âœ… ä¸šç•Œæ ‡å‡†æŒ‡æ ‡

**è¾…åŠ©æŒ‡æ ‡**:

```python
from sklearn.metrics import classification_report, confusion_matrix

# åˆ†ç±»æŠ¥å‘Š
print(classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1']))

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_true, y_pred)
```

---

## 9. å®Œæ•´å»ºæ¨¡Pipelineç¤ºä¾‹

### 9.1 å•æ¨¡å‹Pipelineç¤ºä¾‹

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# 1. åŠ è½½æ•°æ®
train_df = pd.read_csv('data/train.csv')
X = train_df.iloc[:, :-1].values
y = train_df.iloc[:, -1].values

# 2. æ„å»ºPipeline
pipeline = Pipeline([
    ('variance_filter', VarianceThreshold(threshold=0.01)),
    ('scaler', StandardScaler()),
    ('feature_selector', SelectKBest(f_classif, k=500)),
    ('classifier', LogisticRegression(
        penalty='l2', C=1.0, max_iter=1000,
        class_weight='balanced', random_state=42
    ))
])

# 3. äº¤å‰éªŒè¯
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_results = cross_validate(pipeline, X, y, cv=cv, 
                            scoring='roc_auc', return_train_score=True)

print(f"ROC-AUC: {cv_results['test_score'].mean():.4f}")
```

### 9.2 å¤šæ¨¡å‹æ¯”è¾ƒPipeline

```python
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

# å®šä¹‰8ä¸ªæ¨¡å‹
models = {
    'Logistic Regression': LogisticRegression(C=0.1, max_iter=2000, random_state=42),
    'SVM (Linear)': SVC(kernel='linear', C=1.0, probability=True, random_state=42),
    'SVM (RBF)': SVC(kernel='rbf', C=10.0, gamma='scale', probability=True, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=200, max_depth=7, random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=7, weights='distance'),
    'Naive Bayes': GaussianNB(var_smoothing=1e-9),
    'Neural Network (MLP)': MLPClassifier(hidden_layer_sizes=(256, 128, 64), 
                                          alpha=0.001, max_iter=500, 
                                          early_stopping=True, random_state=42)
}

# æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹
results = {}
for name, model in models.items():
    pipeline = Pipeline([
        ('variance_filter', VarianceThreshold(threshold=0.01)),
        ('scaler', StandardScaler()),
        ('feature_selector', SelectKBest(f_classif, k=500)),
        ('classifier', model)
    ])
    
    cv_scores = cross_validate(pipeline, X, y, cv=cv, scoring='roc_auc')
    results[name] = cv_scores['test_score'].mean()
    print(f"{name}: {results[name]:.4f}")

# é€‰æ‹©æœ€ä½³æ¨¡å‹
best_model = max(results, key=results.get)
print(f"\næœ€ä½³æ¨¡å‹: {best_model} (ROC-AUC: {results[best_model]:.4f})")
```

### 9.3 é¢„æµ‹ä¸ä¿å­˜ç»“æœ

```python
# ä½¿ç”¨æœ€ä½³æ¨¡å‹è®­ç»ƒå¹¶é¢„æµ‹
best_pipeline = Pipeline([
    ('variance_filter', VarianceThreshold(threshold=0.01)),
    ('scaler', StandardScaler()),
    ('feature_selector', SelectKBest(f_classif, k=500)),
    ('classifier', models[best_model])
])

best_pipeline.fit(X, y)

# é¢„æµ‹æµ‹è¯•é›†
test_in_df = pd.read_csv('data/test_in_domain.csv')
test_cross_df = pd.read_csv('data/test_cross_domain.csv')

y_pred_in = best_pipeline.predict_proba(test_in_df.values)[:, 1]
y_pred_cross = best_pipeline.predict_proba(test_cross_df.values)[:, 1]

# ä¿å­˜ç»“æœ
pd.DataFrame({'prediction_proba': y_pred_in}).to_csv('predictions_in_domain.csv', index=False)
pd.DataFrame({'prediction_proba': y_pred_cross}).to_csv('predictions_cross_domain.csv', index=False)
print("âœ“ é¢„æµ‹å®Œæˆï¼")
```
```

---

## 10. è¿›é˜¶æŠ€å·§

### 10.1 é›†æˆå­¦ä¹ 

```python
from sklearn.ensemble import VotingClassifier

# è½¯æŠ•ç¥¨é›†æˆå¤šä¸ªå¼ºæ¨¡å‹
ensemble = VotingClassifier(
    estimators=[
        ('lr', LogisticRegression(C=1.0, max_iter=1000)),
        ('svm', SVC(kernel='rbf', probability=True)),
        ('mlp', MLPClassifier(hidden_layer_sizes=(256, 128, 64)))
    ],
    voting='soft'
)
```

### 10.2 è¶…å‚æ•°ä¼˜åŒ–

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'classifier__C': [0.1, 1.0, 10.0],
    'feature_selector__k': [200, 500, 1000]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc')
grid_search.fit(X, y)
```

---

## 11. å¸¸è§é™·é˜±ä¸æ³¨æ„äº‹é¡¹

### âŒ é”™è¯¯åšæ³•

1. **ä¸ä½¿ç”¨äº¤å‰éªŒè¯** - æ ·æœ¬å¤ªå°‘ï¼Œå•æ¬¡åˆ’åˆ†ä¸å¯é 
2. **æ•°æ®æ³„éœ²** - åœ¨å…¨æ•°æ®é›†åšç‰¹å¾é€‰æ‹©åå†åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
3. **ç›´æ¥ä½¿ç”¨é«˜ç»´ç‰¹å¾** - 12700ç»´ä¼šå¯¼è‡´ä¸¥é‡è¿‡æ‹Ÿåˆ
4. **å¿½ç•¥ç±»åˆ«ä¸å¹³è¡¡** - åº”ä½¿ç”¨`class_weight='balanced'`

### âœ… æ­£ç¡®åšæ³•

1. **ä½¿ç”¨Pipeline** - ç‰¹å¾é€‰æ‹©åœ¨CVæ¯ä¸€æŠ˜ä¸­ç‹¬ç«‹è¿›è¡Œ
2. **åˆ†å±‚äº¤å‰éªŒè¯** - ä½¿ç”¨`StratifiedKFold`ä¿æŒç±»åˆ«æ¯”ä¾‹
3. **ç‰¹å¾é™ç»´** - å¿…é¡»å…ˆé™è‡³100-1000ç»´å†å»ºæ¨¡

---

## 12. é¢„æœŸæ€§èƒ½åŸºå‡†

åŸºäºæ•°æ®ç‰¹å¾ï¼Œä»¥ä¸‹æ˜¯8ä¸ªæ¨¡å‹çš„åˆç†æ€§èƒ½é¢„æœŸï¼š

| æ¨¡å‹ | é¢„æœŸCV ROC-AUC | å¤æ‚åº¦ | è®­ç»ƒæ—¶é—´ | æ¨èåº¦ |
|------|----------------|--------|----------|--------|
| é€»è¾‘å›å½’(L2) | 0.70 - 0.80 | ä½ | å¿« | â­â­â­â­â­ |
| SVM (Linear) | 0.72 - 0.82 | ä¸­ | ä¸­ | â­â­â­â­ |
| SVM (RBF) | 0.73 - 0.83 | ä¸­ | ä¸­ | â­â­â­â­â­ |
| éšæœºæ£®æ— | 0.68 - 0.78 | ä¸­ | ä¸­ | â­â­â­â­ |
| æ¢¯åº¦æå‡ | 0.72 - 0.82 | é«˜ | æ…¢ | â­â­â­â­ |
| Kè¿‘é‚» | 0.65 - 0.75 | ä½ | å¿« | â­â­â­ |
| æœ´ç´ è´å¶æ–¯ | 0.68 - 0.76 | ä½ | å¿« | â­â­â­ |
| ç¥ç»ç½‘ç»œ(MLP) | 0.73 - 0.84 | é«˜ | æ…¢ | â­â­â­â­ |
| é›†æˆæ¨¡å‹ | 0.75 - 0.85 | é«˜ | æ…¢ | â­â­â­â­â­ |

**æ€§èƒ½è¯„ä¼°æ ‡å‡†**:
- âœ… CV ROC-AUC âˆˆ [0.70, 0.85]: æ­£å¸¸èŒƒå›´
- âš ï¸ CV ROC-AUC > 0.95: å¾ˆå¯èƒ½è¿‡æ‹Ÿåˆ
- âš ï¸ CV ROC-AUC < 0.65: éœ€é‡æ–°ç‰¹å¾é€‰æ‹©
- âœ… è®­ç»ƒ/æµ‹è¯•å·®è· < 0.05: æ³›åŒ–è‰¯å¥½

---

## 13. æ€»ç»“

### 13.1 æ•°æ®ç‰¹ç‚¹

- âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼ˆæ— ç¼ºå¤±ã€æ— å¸¸é‡ï¼‰
- âš ï¸ é«˜ç»´åº¦æŒ‘æˆ˜ï¼ˆ12,700ç‰¹å¾ï¼‰
- âš ï¸ å°æ ·æœ¬é™åˆ¶ï¼ˆ196ä¸ªï¼‰
- âš ï¸ ä½ä¿¡å™ªæ¯”ï¼ˆ71%ç‰¹å¾ç›¸å…³æ€§<0.1ï¼‰
- âœ… è½»å¾®ç±»åˆ«ä¸å¹³è¡¡ï¼ˆå¯å¤„ç†ï¼‰

### 13.2 æ¨¡å‹é€‰æ‹©ç­–ç•¥

**æ¨èä½¿ç”¨8ä¸ªæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ**:

1. **çº¿æ€§æ¨¡å‹** (3ä¸ª):
   - Logistic Regression - åŸºå‡†æ¨¡å‹ï¼Œå¯è§£é‡Šæ€§å¼º
   - SVM (Linear) - é«˜ç»´å°æ ·æœ¬å‹å¥½
   - SVM (RBF) - å¤„ç†éçº¿æ€§å…³ç³»

2. **æ ‘æ¨¡å‹** (2ä¸ª):
   - Random Forest - ç‰¹å¾é‡è¦æ€§åˆ†æ
   - Gradient Boosting - é«˜æ€§èƒ½é›†æˆ

3. **å®ä¾‹æ¨¡å‹** (2ä¸ª):
   - K-Nearest Neighbors - ç®€å•ç›´è§‚
   - Naive Bayes - å¿«é€ŸåŸºå‡†

4. **ç¥ç»ç½‘ç»œ** (1ä¸ª):
   - MLP (256-128-64) - å¤æ‚éçº¿æ€§å»ºæ¨¡

### 13.3 å…³é”®è¡ŒåŠ¨é¡¹

**å¿…é¡»åš**:
1. âœ… ç‰¹å¾é™ç»´ï¼ˆ12,700 â†’ 500ç»´ï¼‰
2. âœ… 5æŠ˜åˆ†å±‚äº¤å‰éªŒè¯
3. âœ… ä½¿ç”¨Pipelineé˜²æ­¢æ•°æ®æ³„éœ²
4. âœ… æ¯”è¾ƒå¤šä¸ªæ¨¡å‹é€‰æ‹©æœ€ä¼˜

**å»ºè®®åš**:
1. ğŸ”¸ è¶…å‚æ•°è°ƒä¼˜
2. ğŸ”¸ é›†æˆå­¦ä¹ æå‡æ€§èƒ½
3. ğŸ”¸ åˆ†æç‰¹å¾é‡è¦æ€§

### 13.4 ä¸‹ä¸€æ­¥

1. **è¿è¡Œå¤šæ¨¡å‹Pipeline**ï¼ˆè§9.2èŠ‚ï¼‰
2. **é€‰æ‹©æœ€ä½³æ¨¡å‹**
3. **è¿­ä»£ä¼˜åŒ–è¶…å‚æ•°**
4. **ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹**

---

## é™„å½•: å¿«é€Ÿå‚è€ƒ

### æ•°æ®è§„æ¨¡
- æ ·æœ¬: 196 | ç‰¹å¾: 12,700 | ç±»åˆ«: 2 (70/126)

### ç‰¹å¾è´¨é‡
- ä½æ–¹å·®(<0.01): 531 | é«˜ç›¸å…³(>0.2): 365 | æœ€é«˜ç›¸å…³: 0.32

### æ¨èé…ç½®
- **ç‰¹å¾é™ç»´**: 500ç»´ (VarianceThreshold + SelectKBest)
- **äº¤å‰éªŒè¯**: 5æŠ˜StratifiedKFold
- **æ¨¡å‹é€‰æ‹©**: æ¯”è¾ƒ8ä¸ªæ¨¡å‹ï¼ˆLR, SVMÃ—2, RF, GB, KNN, NB, MLPï¼‰
- **è¯„ä¼°æŒ‡æ ‡**: ROC-AUC
- **è¶…å‚æ•°**: è§ç¬¬9.2èŠ‚æ¨¡å‹å®šä¹‰

### æ ¸å¿ƒPipeline
```python
Pipeline([
    ('variance_filter', VarianceThreshold(0.01)),
    ('scaler', StandardScaler()),
    ('selector', SelectKBest(f_classif, k=500)),
    ('classifier', model)  # 8ä¸ªæ¨¡å‹ä¹‹ä¸€
])
```

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2025å¹´12æœˆ  
**åˆ†æå·¥å…·**: Python 3.12 + scikit-learn  
